{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Training Configuration and Environment Setup\n",
    "\n",
    "This section establishes the foundation for the deep learning experiment. It imports necessary libraries, defines the experimental scope (dataset and features), sets up the file system for results, and initializes critical hyperparameters.\n",
    "\n",
    "#### 1. Library Imports\n",
    "The script leverages **TensorFlow/Keras** for building the deep learning architecture. Notably, it imports a diverse set of layersâ€”`Conv2D` (CNN), `LSTM` (RNN), and `MultiHeadAttention`â€”confirming the **hybrid** nature of the model.\n",
    "* **Evaluation:** `sklearn` metrics are imported to calculate Precision, Recall, and F1-score.\n",
    "* **Explainability:** `shap` is imported for feature importance analysis.\n",
    "* **Visualization:** `matplotlib` is set up for plotting training history and interpretation heatmaps.\n",
    "\n",
    "#### 2. Experiment Configuration\n",
    "The script uses constant flags to control the specific experimental setup. By modifying variables like `DATASET` (e.g., `MPOWER_DATASET`) or `MODE`, the user can switch between different training scenarios without altering the core logic.\n",
    "\n",
    "#### 3. Dynamic Path Management\n",
    "To ensure a structured workflow, the code dynamically constructs file paths based on the selected configuration:\n",
    "* **Input Path:** Points to the `.npz` feature file generated by the preprocessing pipeline.\n",
    "* **Output Paths:** Automatically creates directories to separate model checkpoints (`best_model.keras`), evaluation logs, SHAP analysis values, and Grad-CAM heatmaps. This ensures that results from different experiments do not overwrite each other.\n",
    "\n",
    "#### 4. Hyperparameters & Optimization\n",
    "Key training parameters are defined here to control the model's learning process:\n",
    "* **Epochs (30) & Batch Size (32):** Standard settings for convergence on this dataset size.\n",
    "* **Learning Rate (0.001):** A standard starting point for the Adam optimizer.\n",
    "* **Regularization:**\n",
    "    * **Dropout (0.5):** A high dropout rate is used to aggressively prevent overfitting, which is critical for medical datasets with limited samples.\n",
    "    * **L2 Regularization (0.01):** Applied to penalize large weights and further improve generalization.\n",
    "\n",
    "#### 5. Model Checkpointing\n",
    "A `ModelCheckpoint` callback is initialized to monitor the **Validation AUC** (`val_auc`).\n",
    "* **Strategy:** It saves the model weights *only* when the AUC improves.\n",
    "* **Rationale:** In medical diagnostics, where class imbalance is common, maximizing AUC is often more clinically relevant than maximizing simple accuracy."
   ],
   "id": "1292799bc02ae14c"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {},
   "source": [
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import (Input, Conv2D, MaxPooling2D, Dropout, Flatten,\n",
    "                                     Dense, LSTM, MultiHeadAttention, Concatenate, Reshape)\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import ttest_ind\n",
    "import pandas as pd\n",
    "import shap\n",
    "from tensorflow.keras.saving import register_keras_serializable\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import matplotlib.patches as mpatches\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Reshape\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from skimage.transform import resize\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# =============================================================================\n",
    "# --- Configuration ---\n",
    "# =============================================================================\n",
    "\n",
    "# --- 1. Core Paths ---\n",
    "ITALIAN_DATASET = \"ITALIAN_DATASET\"\n",
    "UAMS_DATASET = \"UAMS_DATASET\"\n",
    "MPOWER_DATASET = \"MPOWER_DATASET\"\n",
    "\n",
    "MODE_ALL_VALIDS = \"ALL_VALIDS\"\n",
    "MODE_A = \"A\"\n",
    "\n",
    "FEATURE_MODE_DEFAULT = \"DEFAULT\"\n",
    "\n",
    "MODEL_NAME = \"parallel\"\n",
    "\n",
    "# /////////// SELCET HERE \\\\\\\\\\\\\\\\\\\\\\\n",
    "# ----------------------------------\n",
    "DATASET = MPOWER_DATASET\n",
    "MODE = MODE_A\n",
    "FEATURE_MODE = FEATURE_MODE_DEFAULT\n",
    "# ----------------------------------"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dataset = \"\"\n",
    "elif DATASET == UAMS_DATASET:\n",
    "    dataset = \"UAMS\"\n",
    "elif DATASET == MPOWER_DATASET:\n",
    "    dataset = \"mPower\"\n",
    "elif DATASET == ITALIAN_DATASET:\n",
    "    dataset = \"Italian\"\n",
    "\n",
    "# Path Setup\n",
    "FEATURES_FILE_PATH = os.path.join(os.getcwd(), dataset, \"data\", f\"features_{MODE}_{FEATURE_MODE}.npz\")\n",
    "\n",
    "MODEL_PATH = os.path.join(os.getcwd(), dataset, f\"results_{MODE}_{FEATURE_MODE}\", MODEL_NAME)\n",
    "os.makedirs(MODEL_PATH, exist_ok=True)\n",
    "EVALUATION_FILE_PATH = os.path.join(MODEL_PATH, \"evaluation.csv\")\n",
    "HISTORY_SAVE_PATH = os.path.join(MODEL_PATH, \"history.csv\")\n",
    "BEST_MODEL_PATH = os.path.join(MODEL_PATH, \"best_model.keras\")\n",
    "\n",
    "SHAP_OUTPUT_PATH = os.path.join(MODEL_PATH, \"shap_analysis\")\n",
    "GRADCAM_OUTPUT_PATH = os.path.join(MODEL_PATH, \"gradcam_analysis\")\n",
    "os.makedirs(SHAP_OUTPUT_PATH, exist_ok=True)\n",
    "os.makedirs(GRADCAM_OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "DROPOUT_RATE = 0.5\n",
    "L2_STRENGTH = 0.01\n",
    "\n",
    "# Model Checkpoint Callback\n",
    "checkpoint_cb = ModelCheckpoint(BEST_MODEL_PATH, monitor='val_auc', mode='max', save_best_only=True, verbose=1)"
   ],
   "id": "da1cd86c3e2012d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def load_data(feature_file_path):\n",
    "\n",
    "    print(f\"--- Loading data from {feature_file_path} ---\")\n",
    "    with np.load(feature_file_path) as data:\n",
    "        labels = data['labels']\n",
    "        mel_spectrogram = data['mel_spectrogram']\n",
    "        mfcc = data['mfcc']\n",
    "        X = np.concatenate((mel_spectrogram, mfcc), axis=1)\n",
    "        return X, labels"
   ],
   "id": "e0cb581eacb4685",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Model Architecture: ParkinsonDetectorModel\n",
    "\n",
    "The `ParkinsonDetectorModel` is a custom Keras model designed for the binary classification of Parkinson's Disease using spectral audio features. It employs a **hybrid architecture** that sequentially extracts spatial features using Convolutional Neural Networks (CNNs) and then processes these features in parallel using Recurrent Neural Networks (LSTMs) and Multi-Head Attention mechanisms.\n",
    "\n",
    "#### 1. Class Structure and Initialization\n",
    "The class inherits from `tensorflow.keras.models.Model` and is decorated with `@register_keras_serializable()`, ensuring the custom model structure can be saved and loaded correctly.\n",
    "\n",
    "**Key Components in `__init__`:**\n",
    "* **Input Reshaping:** A `Reshape` layer adds a channel dimension (depth=1) to the 2D input (Mel Spectrograms + MFCCs) to make it compatible with 2D Convolution.\n",
    "* **CNN Blocks:**\n",
    "    * Two consecutive blocks of `Conv2D` layers (64 filters, 5x5 kernel) followed by `MaxPooling2D` and `Dropout`.\n",
    "    * These layers are responsible for learning spatial hierarchies and local patterns in the time-frequency domain.\n",
    "    * L2 regularization (`kernel_regularizer=l2`) is applied to all convolutional layers to prevent overfitting.\n",
    "* **Parallel Branches:**\n",
    "    * **Attention:** A `MultiHeadAttention` layer (2 heads) to identify and weigh critical segments of the feature sequence.\n",
    "    * **LSTM:** A dual-layer `LSTM` stack (128 units) to capture temporal dependencies and long-term evolution of the signal.\n",
    "* **Fusion & Output:**\n",
    "    * `Concatenate` layer to merge the outputs from the CNN, Attention, and LSTM streams.\n",
    "    * A Dense `bottleneck` layer (128 units) for feature compression.\n",
    "    * A final `Dense` output layer with a **Sigmoid** activation for binary probability (Healthy vs. Parkinson's).\n",
    "\n",
    "#### 2. Forward Pass Logic (`call` method)\n",
    "The `call` method defines the data flow through the network:\n",
    "\n",
    "1.  **Feature Extraction (Sequential):**\n",
    "    * The input passes through the two CNN blocks.\n",
    "    * The output `x` (after the second pooling) represents the high-level spatial features.\n",
    "\n",
    "2.  **Branching:**\n",
    "    * **Branch 1 (Spatial):** The CNN output `x` is flattened (`cnn_flat`) to preserve spatial feature information.\n",
    "    * **Sequence Preparation:** The CNN output `x` is reshaped into a sequence format `(batch, time_steps, features)` to serve as input for the temporal branches.\n",
    "\n",
    "3.  **Parallel Processing:**\n",
    "    * **Branch 2 (Attention):** The prepared sequence is passed through Self-Attention (`query=key=value=sequence`). The result highlights important time steps.\n",
    "    * **Branch 3 (Temporal):** The same sequence passes through the LSTM layers to model temporal dynamics.\n",
    "\n",
    "4.  **Fusion and Classification:**\n",
    "    * The outputs of all three branches (`cnn_flat`, `att_flat`, `lstm_out`) are concatenated into a single rich feature vector.\n",
    "    * This vector passes through the bottleneck layer and finally the output layer to produce the prediction.\n",
    "\n",
    "#### 3. Serialization Support\n",
    "The model includes `get_config` and `from_config` methods. These are essential for saving the model's architecture and custom hyperparameters (like `input_shape`) to disk and reconstructing it later without recompiling the code.\n",
    "\n",
    "#### 4. Builder Function\n",
    "The `build_model(input_shape)` helper function wraps this custom class instantiation within the Keras Functional API paradigm. This is standard practice to ensure the model is built and the input shape is correctly inferred before training begins."
   ],
   "id": "5f8dd6fca36758d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# --- Model Architecture ---\n",
    "# =============================================================================\n",
    "\n",
    "@register_keras_serializable()\n",
    "class ParkinsonDetectorModel(Model):\n",
    "    def __init__(self, input_shape, **kwargs):\n",
    "        super(ParkinsonDetectorModel, self).__init__(**kwargs)\n",
    "        self.input_shape_config = input_shape\n",
    "\n",
    "        self.reshape_in = Reshape((input_shape[0], input_shape[1], 1))\n",
    "        self.conv1a = Conv2D(64, 5, activation='relu', kernel_regularizer=l2(L2_STRENGTH), padding='same')\n",
    "        self.conv1b = Conv2D(64, 5, activation='relu', kernel_regularizer=l2(L2_STRENGTH), padding='same')\n",
    "        self.pool1 = MaxPooling2D(5)\n",
    "        self.drop1 = Dropout(DROPOUT_RATE)\n",
    "        self.conv2a = Conv2D(64, 5, activation='relu', kernel_regularizer=l2(L2_STRENGTH), padding='same')\n",
    "        self.conv2b = Conv2D(64, 5, activation='relu', kernel_regularizer=l2(L2_STRENGTH), padding='same', name='last_conv_layer')\n",
    "        self.pool2 = MaxPooling2D(5, name='cnn_output')\n",
    "        self.drop2 = Dropout(DROPOUT_RATE)\n",
    "        self.flatten_cnn = Flatten()\n",
    "        self.attention = MultiHeadAttention(num_heads=2, key_dim=64, name='attention_output')\n",
    "        self.flatten_att = Flatten()\n",
    "        self.lstm1 = LSTM(128, return_sequences=True)\n",
    "        self.lstm2 = LSTM(128, return_sequences=False, name='lstm_output')\n",
    "        self.drop_lstm = Dropout(DROPOUT_RATE)\n",
    "        self.concat = Concatenate()\n",
    "        self.dense_bottleneck = Dense(128, activation='relu', name='bottleneck_features')\n",
    "        self.dense_output = Dense(1, activation='sigmoid')\n",
    "\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.reshape_in(inputs)\n",
    "        x = self.conv1a(x)\n",
    "        x = self.conv1b(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.drop1(x, training=training)\n",
    "        x = self.conv2a(x)\n",
    "        x = self.conv2b(x)\n",
    "        cnn_branch_output = self.pool2(x)\n",
    "        x = self.drop2(cnn_branch_output, training=training)\n",
    "\n",
    "        cnn_flat = self.flatten_cnn(x)\n",
    "\n",
    "        shape = tf.shape(x)\n",
    "        sequence = tf.reshape(x, [-1, shape[1] * shape[2], shape[3]])\n",
    "\n",
    "        att_branch_output = self.attention(query=sequence, key=sequence, value=sequence)\n",
    "        att_flat = self.flatten_att(att_branch_output)\n",
    "\n",
    "        lstm_seq = self.lstm1(sequence)\n",
    "        lstm_branch_output = self.lstm2(lstm_seq)\n",
    "        lstm_out = self.drop_lstm(lstm_branch_output, training=training)\n",
    "\n",
    "        concatenated = self.concat([cnn_flat, att_flat, lstm_out])\n",
    "        bottleneck = self.dense_bottleneck(concatenated)\n",
    "        final_output = self.dense_output(bottleneck)\n",
    "\n",
    "        return final_output\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(ParkinsonDetectorModel, self).get_config()\n",
    "        config.update({\"input_shape\": self.input_shape_config})\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "\n",
    "def build_model(input_shape: tuple) -> Model:\n",
    "    \"\"\"Builds the hybrid model by wrapping the custom class in a Functional API model.\"\"\"\n",
    "    print(\"--- Building the model ---\")\n",
    "    inputs = Input(shape=input_shape)\n",
    "    parkinson_detector = ParkinsonDetectorModel(input_shape=input_shape)\n",
    "    outputs = parkinson_detector(inputs)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    print(\"Model built successfully.\")\n",
    "    return model"
   ],
   "id": "e02a33a578b89d6c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# --- Model Performance ---\n",
    "# =============================================================================\n",
    "def save_metrics_to_csv(y_true, y_pred_proba, filename=\"classification_report.csv\", threshold=0.5):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred_binary = (np.array(y_pred_proba) > threshold).astype(int)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred_binary)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    total_samples = cm.sum()\n",
    "    if total_samples == 0:\n",
    "        tn_percent, fp_percent, fn_percent, tp_percent = 0, 0, 0, 0\n",
    "    else:\n",
    "        tn_percent = (tn / total_samples) * 100\n",
    "        fp_percent = (fp / total_samples) * 100\n",
    "        fn_percent = (fn / total_samples) * 100\n",
    "        tp_percent = (tp / total_samples) * 100\n",
    "\n",
    "    precision = precision_score(y_true, y_pred_binary, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred_binary, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred_binary, zero_division=0)\n",
    "    sensitivity = recall\n",
    "\n",
    "    report_data = {\n",
    "        'Metric': [\n",
    "            'True Positive (TP)',\n",
    "            'True Negative (TN)',\n",
    "            'False Positive (FP)',\n",
    "            'False Negative (FN)',\n",
    "            'Precision',\n",
    "            'Recall (Sensitivity)',\n",
    "            'F1-Score'\n",
    "        ],\n",
    "        'Value': [\n",
    "            f\"{tp} ({tp_percent:.2f}%)\",\n",
    "            f\"{tn} ({tn_percent:.2f}%)\",\n",
    "            f\"{fp} ({fp_percent:.2f}%)\",\n",
    "            f\"{fn} ({fn_percent:.2f}%)\",\n",
    "            f\"{precision:.4f}\",\n",
    "            f\"{recall:.4f}\",\n",
    "            f\"{f1:.4f}\"\n",
    "        ]\n",
    "    }\n",
    "    df = pd.DataFrame(report_data)\n",
    "\n",
    "    try:\n",
    "        df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "        print(f\"The evaluation results is stored: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erorr while saving the evaluation report: {e}\")\n"
   ],
   "id": "20f69c55a6be82dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Feature Extraction for Dimensionality Reduction Analysis\n",
    "\n",
    "This module is designed to extract high-dimensional feature vectors from the internal layers of the trained model. Specifically, it targets the point where features from the CNN, LSTM, and Attention branches are concatenated, just before they enter the final dense classification layers. These extracted vectors are essential for visualizing the model's learned representation space using techniques like PCA, t-SNE, or LDA.\n",
    "\n",
    "#### Class: `DenseLayerInputExtractor`\n",
    "\n",
    "This class handles the creation of a sub-model that isolates the intermediate output of the network.\n",
    "\n",
    "**1. Initialization (`__init__`)**\n",
    "The constructor initializes the class and immediately calls `_setup_extraction_model` to prepare the internal Keras model for feature extraction.\n",
    "\n",
    "**2. Model Setup (`_setup_extraction_model`)**\n",
    "This is the core logic that reconstructs the forward pass of the model up to the concatenation layer.\n",
    "* **Layer Identification:** It iterates through the trained model's layers to find the specific `ParkinsonDetectorModel` instance.\n",
    "* **Graph Reconstruction:** It re-defines the flow of tensors through the specific branches:\n",
    "    * **CNN Branch:** Passes input through convolutional and pooling layers to extract spatial features (`cnn_flat`).\n",
    "    * **Sequence Reshaping:** Reshapes the CNN output to be compatible with sequence processing.\n",
    "    * **Attention Branch:** Computes self-attention on the sequence (`att_flat`).\n",
    "    * **LSTM Branch:** Processes the sequence through LSTM layers to capture temporal dynamics (`lstm_out`).\n",
    "* **Concatenation:** Merges the outputs (`cnn_flat`, `att_flat`, `lstm_out`) into a single vector.\n",
    "* **Model Creation:** Instantiates a new Keras `Model` (`self.extraction_model`) where the input matches the original model, but the output is this concatenated feature vector.\n",
    "\n",
    "**3. Feature Extraction (`extract_and_save_features`)**\n",
    "This method executes the extraction on a dataset and saves the results.\n",
    "* **Sampling:** Allows extracting features from a subset of the data (e.g., 200 random samples) to reduce computational load for visualization, or processing the entire dataset. It handles random seeding for reproducibility.\n",
    "* **Batch Processing:** Iterates through the data in batches to manage memory usage efficiently.\n",
    "* **Output:** Compiles the extracted feature vectors and their corresponding labels into a dictionary and saves them as a compressed NumPy file (`.npz`). This file contains:\n",
    "    * `concatenated_features`: The high-dimensional feature vectors.\n",
    "    * `labels`: The ground truth class labels.\n",
    "    * `sample_indices`: Indices of the samples used.\n",
    "\n",
    "#### Helper Function: `extract_dense_layer_input_features`\n",
    "\n",
    "This standalone function serves as a simplified entry point to the class.\n",
    "* **Workflow:** It instantiates the `DenseLayerInputExtractor`.\n",
    "* **Execution:** It calls `extract_and_save_features` with the test data and specifies the output filename (`dense_layer_input_features.npz`).\n",
    "* **Purpose:** It encapsulates the extraction logic into a single function call for use in the main execution pipeline."
   ],
   "id": "6975ec62bd0dfcd6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class DenseLayerInputExtractor:\n",
    "    \"\"\"Extracts features that go into the dense layer for dimensionality reduction visualization\"\"\"\n",
    "\n",
    "    def __init__(self, trained_model):\n",
    "        self.parkinson_detector = None\n",
    "        self.extraction_model = None\n",
    "        self._setup_extraction_model(trained_model)\n",
    "\n",
    "    def _setup_extraction_model(self, trained_model):\n",
    "        \"\"\"Setup model to extract concatenated features (dense layer input)\"\"\"\n",
    "        print(\"Setting up dense layer input extraction model...\")\n",
    "\n",
    "        # Find the ParkinsonDetectorModel layer\n",
    "        for layer in trained_model.layers:\n",
    "            if isinstance(layer, ParkinsonDetectorModel):\n",
    "                self.parkinson_detector = layer\n",
    "                break\n",
    "\n",
    "        if self.parkinson_detector is None:\n",
    "            raise ValueError(\"ParkinsonDetectorModel not found in the trained model\")\n",
    "\n",
    "        def extract_dense_input_features(inputs):\n",
    "            \"\"\"Extract the concatenated features that go into the dense layer\"\"\"\n",
    "            # Pass through CNN branch\n",
    "            x = self.parkinson_detector.reshape_in(inputs)\n",
    "            x = self.parkinson_detector.conv1a(x)\n",
    "            x = self.parkinson_detector.conv1b(x)\n",
    "            x = self.parkinson_detector.pool1(x)\n",
    "            x_dropped = self.parkinson_detector.drop1(x, training=False)\n",
    "            x = self.parkinson_detector.conv2a(x_dropped)\n",
    "            x = self.parkinson_detector.conv2b(x)\n",
    "            cnn_branch_output = self.parkinson_detector.pool2(x)\n",
    "            x_dropped = self.parkinson_detector.drop2(cnn_branch_output, training=False)\n",
    "\n",
    "            # Prepare sequence for attention and LSTM\n",
    "            def reshape_for_sequence(x):\n",
    "                shape = tf.shape(x)\n",
    "                return tf.reshape(x, [-1, shape[1] * shape[2], shape[3]])\n",
    "\n",
    "            sequence = Lambda(reshape_for_sequence, name='sequence_reshape')(x_dropped)\n",
    "\n",
    "            # Attention branch\n",
    "            att_branch_output = self.parkinson_detector.attention(\n",
    "                query=sequence, key=sequence, value=sequence\n",
    "            )\n",
    "\n",
    "            # LSTM branch\n",
    "            lstm_seq = self.parkinson_detector.lstm1(sequence)\n",
    "            lstm_branch_output = self.parkinson_detector.lstm2(lstm_seq)\n",
    "            lstm_out = self.parkinson_detector.drop_lstm(lstm_branch_output, training=False)\n",
    "\n",
    "            # Flatten for concatenation\n",
    "            cnn_flat = self.parkinson_detector.flatten_cnn(x_dropped)\n",
    "            att_flat = self.parkinson_detector.flatten_att(att_branch_output)\n",
    "\n",
    "            # This is what you need - the concatenated features (dense layer input)\n",
    "            concatenated_features = self.parkinson_detector.concat([cnn_flat, att_flat, lstm_out])\n",
    "\n",
    "            return concatenated_features\n",
    "\n",
    "        # Create the extraction model\n",
    "        inputs = Input(shape=trained_model.input_shape[1:])\n",
    "        dense_input_features = extract_dense_input_features(inputs)\n",
    "        self.extraction_model = Model(inputs=inputs, outputs=dense_input_features)\n",
    "\n",
    "        print(\"âœ… Dense layer input extraction model ready\")\n",
    "\n",
    "    def extract_and_save_features(self, X_data, y_data, output_path, sample_indices=None, batch_size=32):\n",
    "        \"\"\"Extract and save concatenated features (dense layer input)\"\"\"\n",
    "\n",
    "        print(f\"Input data shape: {X_data.shape}\")\n",
    "\n",
    "        # Handle sample selection\n",
    "        if sample_indices is None:\n",
    "            sample_indices = np.arange(len(X_data))\n",
    "            print(\"Using all available samples\")\n",
    "        elif isinstance(sample_indices, int):\n",
    "            available_samples = len(X_data)\n",
    "            requested_samples = sample_indices\n",
    "\n",
    "            if requested_samples > available_samples:\n",
    "                print(f\"âš ï¸  Warning: Requested {requested_samples} samples, but only {available_samples} available.\")\n",
    "                print(f\"   Using all {available_samples} samples instead.\")\n",
    "                sample_indices = np.arange(available_samples)\n",
    "            else:\n",
    "                np.random.seed(42)\n",
    "                sample_indices = np.random.choice(available_samples, requested_samples, replace=False)\n",
    "                print(f\"Randomly selected {requested_samples} samples from {available_samples} available.\")\n",
    "\n",
    "        print(f\"Processing {len(sample_indices)} samples...\")\n",
    "\n",
    "        # Extract concatenated features in batches\n",
    "        all_concatenated_features = []\n",
    "\n",
    "        for i in tqdm(range(0, len(sample_indices), batch_size), desc=\"Extracting dense layer input features\"):\n",
    "            batch_indices = sample_indices[i:i+batch_size]\n",
    "            batch_X = X_data[batch_indices]\n",
    "\n",
    "            # Extract concatenated features for this batch\n",
    "            batch_features = self.extraction_model.predict(batch_X, verbose=0)\n",
    "            all_concatenated_features.append(batch_features)\n",
    "\n",
    "        # Concatenate all batches\n",
    "        concatenated_features = np.concatenate(all_concatenated_features, axis=0)\n",
    "        print(f\"Concatenated features shape: {concatenated_features.shape}\")\n",
    "\n",
    "        # Prepare data for your visualization script\n",
    "        features_dict = {\n",
    "            'concatenated_features': concatenated_features,  # This is what your viz script needs\n",
    "            'labels': y_data[sample_indices],\n",
    "            'sample_indices': sample_indices\n",
    "        }\n",
    "\n",
    "        # Save features\n",
    "        np.savez_compressed(output_path, **features_dict)\n",
    "        print(f\"âœ… Dense layer input features saved to: {output_path}\")\n",
    "\n",
    "        return output_path\n",
    "\n",
    "def extract_dense_layer_input_features(trained_model, X_test, y_test, output_dir, num_samples=200):\n",
    "    \"\"\"Extract concatenated features that serve as input to the dense layer\"\"\"\n",
    "\n",
    "    print(\"ðŸ”¬ EXTRACTING DENSE LAYER INPUT FEATURES...\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Create extractor\n",
    "    extractor = DenseLayerInputExtractor(trained_model)\n",
    "\n",
    "    # Extract and save features\n",
    "    output_path = os.path.join(output_dir, \"dense_layer_input_features.npz\")\n",
    "    feature_file = extractor.extract_and_save_features(\n",
    "        X_data=X_test,\n",
    "        y_data=y_test,\n",
    "        output_path=output_path,\n",
    "        sample_indices=num_samples\n",
    "    )\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"âœ… DENSE LAYER INPUT FEATURE EXTRACTION COMPLETED!\")\n",
    "\n",
    "    return feature_file\n"
   ],
   "id": "d901a4551da9e499",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Comprehensive Visualization and Analysis Tools\n",
    "\n",
    "This section introduces advanced visualization functions designed to produce high-quality, publication-ready figures for analyzing model performance and feature separability. These functions are optimized for creating clear, informative plots suitable for posters and presentations.\n",
    "\n",
    "#### 1. Feature Separability Analysis (`analyze_feature_separability`)\n",
    "This function visualizes how well the model's learned features separate the two classes (Healthy vs. Parkinson's) using three dimensionality reduction techniques. It is optimized for large displays (XL/Poster version).\n",
    "\n",
    "* **Dimensionality Reduction Techniques:**\n",
    "    * **PCA (Principal Component Analysis):** Projects features onto the two principal components that capture the most variance.\n",
    "    * **LDA (Linear Discriminant Analysis):** Projects features onto a single axis that maximizes class separation. The results are plotted as **density curves (KDE)** to show the distribution overlap.\n",
    "    * **t-SNE (t-Distributed Stochastic Neighbor Embedding):** Visualizes high-dimensional clusters in 2D. A subset of samples (up to 1000) is used for efficiency.\n",
    "\n",
    "* **Quantitative Metric (d-prime):**\n",
    "    * Calculates the **d-prime** sensitivity index based on the LDA projection.\n",
    "    * **Interpretation:** A higher d-prime indicates better separability between the healthy and pathological distributions. This value is annotated directly on the LDA plot.\n",
    "\n",
    "* **Visualization Details:**\n",
    "    * Generates a side-by-side subplot of PCA, LDA, and t-SNE.\n",
    "    * Uses distinct colors (Blue for Healthy, Red for Parkinson's) and large fonts for readability.\n",
    "\n",
    "#### 2. Side-by-Side Confusion Matrix & ROC (`plot_cm_and_roc_side_by_side`)\n",
    "This function creates a unified figure combining the two most critical binary classification metrics.\n",
    "\n",
    "* **Confusion Matrix (Left Panel):**\n",
    "    * Displays raw counts and percentages for True Negatives, False Positives, False Negatives, and True Positives.\n",
    "    * Annotates the axes with calculated **Specificity** and **Sensitivity (Recall)** scores.\n",
    "    * Uses a heatmap with large annotations for clear visibility.\n",
    "\n",
    "* **ROC Curve (Right Panel):**\n",
    "    * Plots the Receiver Operating Characteristic curve.\n",
    "    * Displays the **AUC (Area Under the Curve)** score in the legend.\n",
    "    * Includes a diagonal reference line (random guess) for comparison.\n",
    "\n",
    "#### 3. Full 3-Panel Evaluation (`plot_full_evaluation_square`)\n",
    "This comprehensive function generates a single figure containing three key evaluation plots, maintaining a square aspect ratio for better visual balance.\n",
    "\n",
    "* **Panel 1: Confusion Matrix:** Same enhanced heatmap as above.\n",
    "* **Panel 2: ROC Curve:** Standard ROC plot with AUC.\n",
    "* **Panel 3: Training History:**\n",
    "    * Plots **Loss** (Training vs. Validation) on the primary y-axis (red).\n",
    "    * Plots **AUC** (Training vs. Validation) on the secondary y-axis (blue).\n",
    "    * This allows for simultaneous monitoring of convergence and overfitting (e.g., if validation loss starts rising while training loss falls).\n",
    "\n",
    "All functions automatically save the generated high-resolution (300 DPI) images to the specified output directory."
   ],
   "id": "ac6195b252b3f3d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# --- New Imports for Enhanced Analysis and Plotting ---\n",
    "# =============================================================================\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# =============================================================================\n",
    "# --- Feature Separability Analysis Function (Poster - XL Version) ---\n",
    "# =============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.manifold import TSNE\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.manifold import TSNE\n",
    "import os\n",
    "\n",
    "def analyze_feature_separability(features, labels, title_prefix, output_path):\n",
    "    \"\"\"\n",
    "    Performs and plots PCA, LDA (as density curves), and t-SNE.\n",
    "    Calculates and adds a large d-prime separability score to the LDA plot.\n",
    "    This version is optimized for extra-large display (e.g., posters).\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Analyzing Separability for {title_prefix} ---\")\n",
    "    # Ensure labels are 1D array\n",
    "    labels = np.array(labels).flatten()\n",
    "\n",
    "    # Reshape features to 2D (samples, flattened_features)\n",
    "    if features.ndim > 2:\n",
    "        features_2d = features.reshape(features.shape[0], -1)\n",
    "    else:\n",
    "        features_2d = features\n",
    "\n",
    "    print(f\"Feature shape for analysis: {features_2d.shape}\")\n",
    "\n",
    "    # --- Apply Dimensionality Reduction ---\n",
    "    pca = PCA(n_components=2)\n",
    "    features_pca = pca.fit_transform(features_2d)\n",
    "\n",
    "    lda = LinearDiscriminantAnalysis(n_components=1)\n",
    "    features_lda = lda.fit_transform(features_2d, labels)\n",
    "\n",
    "    # Use a smaller subset for t-SNE if the dataset is large to speed it up\n",
    "    tsne_samples = min(1000, features_2d.shape[0])\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, tsne_samples - 1))\n",
    "\n",
    "    if features_2d.shape[0] < tsne.perplexity + 1:\n",
    "        print(f\"Warning: Not enough samples ({features_2d.shape[0]}) for t-SNE with perplexity {tsne.perplexity}. Skipping t-SNE.\")\n",
    "        features_tsne = None\n",
    "        tsne_labels = None\n",
    "    else:\n",
    "        features_tsne = tsne.fit_transform(features_2d[:tsne_samples, :])\n",
    "        tsne_labels = labels[:tsne_samples]\n",
    "\n",
    "    # --- Plotting ---\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(36, 12))\n",
    "    fig.suptitle(f'Feature Separability Analysis: {title_prefix}', fontsize=32, fontweight='bold', y=1.03)\n",
    "\n",
    "    # Define color, label, and ORDER mappings\n",
    "    class_label_map = {0: 'Healthy', 1: 'Parkinson'}\n",
    "    palette = {'Healthy': 'blue', 'Parkinson': 'red'}\n",
    "    legend_order = ['Healthy', 'Parkinson'] # <<< ADD THIS LINE TO DEFINE ORDER\n",
    "\n",
    "    # Map numeric labels to string labels for plotting\n",
    "    mapped_labels = np.array([class_label_map[l] for l in labels])\n",
    "\n",
    "    # PCA Plot\n",
    "    sns.scatterplot(x=features_pca[:, 0], y=features_pca[:, 1], hue=mapped_labels,\n",
    "                    palette=palette, ax=axes[0], s=150, alpha=0.8,\n",
    "                    hue_order=legend_order) # <<< ADD hue_order HERE\n",
    "    axes[0].set_title('PCA Projection', fontsize=26)\n",
    "    axes[0].set_xlabel('Principal Component 1', fontsize=22)\n",
    "    axes[0].set_ylabel('Principal Component 2', fontsize=22)\n",
    "    axes[0].tick_params(axis='both', which='major', labelsize=18)\n",
    "    axes[0].legend(title='Class', fontsize=18, title_fontsize=20)\n",
    "\n",
    "    # LDA Plot (Density Curves)\n",
    "    df_lda = pd.DataFrame({'LDA Projection': features_lda.flatten(), 'label': mapped_labels})\n",
    "    sns.kdeplot(data=df_lda, x='LDA Projection', hue='label', palette=palette,\n",
    "                ax=axes[1], fill=True, common_norm=False, linewidth=3.5,\n",
    "                hue_order=legend_order) # <<< ADD hue_order HERE\n",
    "    axes[1].set_title('LDA Projection (Density)', fontsize=26)\n",
    "    axes[1].set_xlabel('LDA Projection', fontsize=22)\n",
    "    axes[1].set_ylabel('Density', fontsize=22)\n",
    "    axes[1].tick_params(axis='both', which='major', labelsize=18)\n",
    "    axes[1].legend(title='Class', fontsize=18, title_fontsize=20)\n",
    "\n",
    "\n",
    "    # Calculate and add d-prime to the LDA plot\n",
    "    mean_0 = np.mean(features_lda[labels == 0])\n",
    "    mean_1 = np.mean(features_lda[labels == 1])\n",
    "    std_0 = np.std(features_lda[labels == 0])\n",
    "    std_1 = np.std(features_lda[labels == 1])\n",
    "\n",
    "    if std_0 > 0 and std_1 > 0:\n",
    "        d_prime = (2 * (mean_1 - mean_0)) / np.sqrt(std_0**2 + std_1**2)\n",
    "        print(f\"Separability Metric (d-prime based on LDA): {d_prime:.4f}\")\n",
    "        axes[1].text(0.05, 0.95, f\"d-prime: {d_prime:.4f}\",\n",
    "                     transform=axes[1].transAxes, fontsize=24, fontweight='bold',\n",
    "                     verticalalignment='top', bbox=dict(boxstyle='round,pad=1.0', fc='yellow', alpha=0.7))\n",
    "    else:\n",
    "        print(\"Could not calculate d-prime (standard deviation is zero for one class).\")\n",
    "        axes[1].text(0.05, 0.95, \"d-prime: N/A\",\n",
    "                     transform=axes[1].transAxes, fontsize=24, fontweight='bold',\n",
    "                     verticalalignment='top', bbox=dict(boxstyle='round,pad=1.0', fc='gray', alpha=0.7))\n",
    "\n",
    "    # t-SNE Plot\n",
    "    if features_tsne is not None:\n",
    "        mapped_tsne_labels = np.array([class_label_map[l] for l in tsne_labels])\n",
    "        sns.scatterplot(x=features_tsne[:, 0], y=features_tsne[:, 1], hue=mapped_tsne_labels,\n",
    "                        palette=palette, ax=axes[2], s=150, alpha=0.8,\n",
    "                        hue_order=legend_order) # <<< ADD hue_order HERE\n",
    "        axes[2].set_title('t-SNE Projection', fontsize=26)\n",
    "        axes[2].set_xlabel('t-SNE Dimension 1', fontsize=22)\n",
    "        axes[2].set_ylabel('t-SNE Dimension 2', fontsize=22)\n",
    "        axes[2].tick_params(axis='both', which='major', labelsize=18)\n",
    "        axes[2].legend(title='Class', fontsize=18, title_fontsize=20)\n",
    "    else:\n",
    "        axes[2].set_title('t-SNE Projection (Skipped)', fontsize=26)\n",
    "        axes[2].text(0.5, 0.5, \"Not enough samples for t-SNE\",\n",
    "                     transform=axes[2].transAxes, ha='center', va='center', fontsize=18, color='gray')\n",
    "        axes[2].set_xticks([])\n",
    "        axes[2].set_yticks([])\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.98])\n",
    "    save_path = os.path.join(output_path, f'{title_prefix.lower().replace(\" \", \"_\")}_separability_analysis_poster_XL.png')\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.show()\n",
    "    print(f\"Extra-large separability plots for poster saved to: {save_path}\")\n",
    "\n",
    "# =============================================================================\n",
    "# --- Side-by-Side Confusion Matrix and ROC Curve Plotting (Poster Version) ---\n",
    "# =============================================================================\n",
    "def plot_cm_and_roc_side_by_side(y_true, y_pred_proba, output_path, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Creates a single, large figure with the enhanced confusion matrix and ROC curve\n",
    "    plotted side-by-side, suitable for posters and presentations.\n",
    "    \"\"\"\n",
    "    print(\"\\\\n--- Generating Poster-Sized Side-by-Side CM and ROC Plot ---\")\n",
    "    y_true = np.array(y_true).flatten()\n",
    "    y_pred_proba = np.array(y_pred_proba).flatten()\n",
    "    y_pred_binary = (y_pred_proba > threshold).astype(int)\n",
    "\n",
    "    # --- Create Larger Figure ---\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(22, 9))\n",
    "    fig.suptitle('Model Evaluation: Confusion Matrix & ROC Curve', fontsize=28, fontweight='bold', y=1.02)\n",
    "\n",
    "    # --- 1. Enhanced Confusion Matrix (Left Subplot) ---\n",
    "    cm = confusion_matrix(y_true, y_pred_binary)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    labels = np.asarray([\n",
    "        [f\"{tn} ({cm_percent[0,0]:.2%})\", f\"{fp} ({cm_percent[0,1]:.2%})\"],\n",
    "        [f\"{fn} ({cm_percent[1,0]:.2%})\", f\"{tp} ({cm_percent[1,1]:.2%})\"]\n",
    "    ])\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "\n",
    "    sns.heatmap(cm, annot=labels, fmt='', cmap='Blues', cbar=False, ax=ax1,\n",
    "                xticklabels=['Predicted Healthy', 'Predicted Parkinson'],\n",
    "                yticklabels=['Actual Healthy', 'Actual Parkinson'],\n",
    "                annot_kws={\"size\": 24}) # <<< INCREASED FONT SIZE HERE\n",
    "\n",
    "    ax1.set_title('Confusion Matrix', fontsize=22)\n",
    "    ax1.set_xlabel(f'Specificity: {specificity:.4f}', fontsize=18)\n",
    "    ax1.set_ylabel(f'Sensitivity (Recall): {sensitivity:.4f}', fontsize=18)\n",
    "    ax1.tick_params(axis='both', which='major', labelsize=16)\n",
    "\n",
    "    # --- 2. ROC Curve (Right Subplot) ---\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    ax2.plot(fpr, tpr, color='red', lw=3, label=f'ROC Curve (AUC = {roc_auc:.3f})')\n",
    "    ax2.plot([0, 1], [0, 1], color='black', lw=2, linestyle='--')\n",
    "    ax2.set_xlim([0.0, 1.0])\n",
    "    ax2.set_ylim([0.0, 1.0])\n",
    "    ax2.set_xlabel('False Positive Rate', fontsize=18)\n",
    "    ax2.set_ylabel('True Positive Rate', fontsize=18)\n",
    "    ax2.set_title('Receiver Operating Characteristic (ROC) Curve', fontsize=22)\n",
    "    ax2.legend(loc=\"lower right\", fontsize=16)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.tick_params(axis='both', which='major', labelsize=14)\n",
    "\n",
    "    # --- Save and Show ---\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.98])\n",
    "    save_path = os.path.join(output_path, 'cm_roc_side_by_side_poster.png')\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.show()\n",
    "    print(f\"Poster-sized side-by-side plot saved to: {save_path}\")"
   ],
   "id": "35686588f81f7d39",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "\n",
    "# =============================================================================\n",
    "# --- Comprehensive 3-Plot Evaluation (CM, ROC, and Training History) ---\n",
    "# =============================================================================\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "\n",
    "# =============================================================================\n",
    "# --- Comprehensive 3-Plot Evaluation (CM, ROC, and Training History) ---\n",
    "# =============================================================================\n",
    "def plot_full_evaluation_square(history, y_true, y_pred_proba, output_path, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Creates a single figure with three plots side-by-side, aiming for a more\n",
    "    square aspect ratio for each, especially CM and ROC.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Generating Comprehensive 3-Plot Evaluation Figure (Square Aspect) ---\")\n",
    "\n",
    "    # --- Prepare Data ---\n",
    "    y_true = np.array(y_true).flatten()\n",
    "    y_pred_proba = np.array(y_pred_proba).flatten()\n",
    "    y_pred_binary = (y_pred_proba > threshold).astype(int)\n",
    "    hist_df = pd.DataFrame(history.history)\n",
    "    epochs = range(1, len(hist_df) + 1)\n",
    "\n",
    "    # --- Create Larger 3-Panel Figure with Square Aspect ---\n",
    "    # We'll use gridspec_kw to ensure the aspect ratio of the first two plots\n",
    "    # We want each subplot area to be roughly square, so adjust figsize accordingly.\n",
    "    # For three square plots, a width of ~3*height is a good start.\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(27, 9)) # Adjust figsize, keeping height fixed\n",
    "    fig.suptitle('Comprehensive Model Evaluation', fontsize=28, fontweight='bold', y=1.02)\n",
    "\n",
    "    # --- PLOT 1: Enhanced Confusion Matrix (Left) ---\n",
    "    cm = confusion_matrix(y_true, y_pred_binary)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    labels = np.asarray([\n",
    "        [f\"{tn}\\n({cm_percent[0,0]:.2%})\", f\"{fp}\\n({cm_percent[0,1]:.2%})\"],\n",
    "        [f\"{fn}\\n({cm_percent[1,0]:.2%})\", f\"{tp}\\n({cm_percent[1,1]:.2%})\"]\n",
    "    ])\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "\n",
    "    sns.heatmap(cm, annot=labels, fmt='', cmap='Blues', cbar=False, ax=ax1,\n",
    "                xticklabels=['Predicted Healthy', 'Predicted Parkinson'],\n",
    "                yticklabels=['Actual Healthy', 'Actual Parkinson'],\n",
    "                annot_kws={\"size\": 24})\n",
    "    ax1.set_title('Confusion Matrix', fontsize=22)\n",
    "    ax1.set_xlabel(f'Specificity: {specificity:.4f}', fontsize=18)\n",
    "    ax1.set_ylabel(f'Sensitivity (Recall): {sensitivity:.4f}', fontsize=18)\n",
    "    ax1.tick_params(axis='both', which='major', labelsize=16)\n",
    "    ax1.set_aspect('equal', adjustable='box') # Force square aspect\n",
    "\n",
    "    # --- PLOT 2: ROC Curve (Middle) ---\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    ax2.plot(fpr, tpr, color='red', lw=3, label=f'ROC Curve (AUC = {roc_auc:.3f})')\n",
    "    ax2.plot([0, 1], [0, 1], color='black', lw=2, linestyle='--')\n",
    "    ax2.set_xlim([0.0, 1.0])\n",
    "    ax2.set_ylim([0.0, 1.0])\n",
    "    ax2.set_xlabel('False Positive Rate', fontsize=18)\n",
    "    ax2.set_ylabel('True Positive Rate', fontsize=18)\n",
    "    ax2.set_title('Receiver Operating Characteristic', fontsize=22)\n",
    "    ax2.legend(loc=\"lower right\", fontsize=16)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.tick_params(axis='both', which='major', labelsize=14)\n",
    "    ax2.set_aspect('equal', adjustable='box') # Force square aspect\n",
    "\n",
    "    # --- PLOT 3: Training History (Right) ---\n",
    "    # For the history plot, we maintain its current aspect as forcing 'equal'\n",
    "    # would distort the time-series nature of loss and AUC.\n",
    "    # Instead, we ensure its overall subplot area is square, and its internal\n",
    "    # content will fill that area appropriately.\n",
    "    ax3.set_title('Training History', fontsize=22)\n",
    "    ax3.set_xlabel('Epoch', fontsize=18)\n",
    "\n",
    "    # Plot Loss on the primary y-axis\n",
    "    ax3_loss = ax3\n",
    "    color = 'tab:red'\n",
    "    ax3_loss.set_ylabel('Loss', color=color, fontsize=18)\n",
    "    p1 = ax3_loss.plot(epochs, hist_df['loss'], color=color, linestyle='-', label='Training Loss')\n",
    "    p2 = ax3_loss.plot(epochs, hist_df['val_loss'], color=color, linestyle='--', label='Validation Loss')\n",
    "    ax3_loss.tick_params(axis='y', labelcolor=color, labelsize=14)\n",
    "\n",
    "    # Plot AUC on the secondary y-axis\n",
    "    ax3_auc = ax3.twinx()\n",
    "    color = 'tab:blue'\n",
    "    ax3_auc.set_ylabel('AUC', color=color, fontsize=18)\n",
    "    p3 = ax3_auc.plot(epochs, hist_df['auc'], color=color, linestyle='-', label='Training AUC')\n",
    "    p4 = ax3_auc.plot(epochs, hist_df['val_auc'], color=color, linestyle='--', label='Validation AUC')\n",
    "    ax3_auc.tick_params(axis='y', labelcolor=color, labelsize=14)\n",
    "\n",
    "    # Unified legend for the third plot\n",
    "    lines = p1 + p2 + p3 + p4\n",
    "    labels = [l.get_label() for l in lines]\n",
    "    ax3.legend(lines, labels, loc='center right', fontsize=14)\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    # ax3.set_aspect('equal', adjustable='box') # Only if you truly want to distort it. Not recommended here.\n",
    "\n",
    "    # --- Save and Show ---\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.98])\n",
    "    save_path = os.path.join(output_path, 'full_evaluation_report_square.png')\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.show()\n",
    "    print(f\"Comprehensive evaluation plot (square aspect) saved to: {save_path}\")"
   ],
   "id": "fdc7b1ec2916c9fd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Model Explainability: Interpreting Decisions\n",
    "\n",
    "To move beyond \"black box\" predictions, this module implements techniques to visualize *what* the model is looking at. It specifically helps diagnose issues like \"tunnel vision\" (where a model might over-focus on a recording artifact rather than the voice itself).\n",
    "\n",
    "#### 1. Feature Map Layout (`generate_feature_map_info`)\n",
    "Since the input to the model is a concatenated matrix of different features (e.g., Mel Spectrogram rows 0-29, MFCC rows 30-59), we need a \"map\" to interpret the results.\n",
    "* **Introspection:** It loads the original `.npz` feature file and inspects the shapes of the arrays inside.\n",
    "* **Layout Generation:** It dynamically creates a color-coded mask that maps specific rows of the input matrix back to their feature names (e.g., \"Rows 0-30 are Mel Spectrogram\").\n",
    "* **Visualization:** This allows us to overlay a legend on our heatmaps, clearly showing which biological feature corresponds to a high-attention region.\n",
    "\n",
    "#### 2. Grad-CAM Analysis (`run_gradcam_analysis`)\n",
    "This function performs **Gradient-weighted Class Activation Mapping** to visualize the spatial attention of the CNN branch.\n",
    "\n",
    "* **Backpropagation Logic:**\n",
    "    1.  It selects a random subset of **True Positive (Parkinson's)** and **True Negative (Healthy)** samples.\n",
    "    2.  For each sample, it computes the gradient of the predicted class score with respect to the feature maps of the *last convolutional layer* (`conv2b`).\n",
    "    3.  These gradients act as weights: high gradients mean that specific region was crucial for the decision.\n",
    "    4.  The weighted feature maps are summed to create a coarse \"heatmap.\"\n",
    "\n",
    "* **Upscaling:** The coarse heatmap (from the deep, pooled layer) is resized back to the original input dimensions (`194 x 60`) so it can be overlaid on the input features.\n",
    "\n",
    "* **Visualization (The 3-Panel Plot):**\n",
    "    * **Panel 1 (PD Attention):** Average heatmap for Parkinson's patients. Hot spots (red) show where the model looks to find disease indicators.\n",
    "    * **Panel 2 (Healthy Attention):** Average heatmap for healthy controls.\n",
    "    * **Panel 3 (Feature Map):** The color-coded legend generated earlier, transposed to match the orientation of the heatmaps (Time on Y-axis, Features on X-axis).\n",
    "\n",
    "**Why this matters:** If the \"PD Attention\" map shows a distinct vertical line at the very start of the audio, it proves the model is suffering from the \"tunnel vision\" artifact described in the thesis."
   ],
   "id": "83675717e973aa05"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# --- Model Explainability (SHAP & Grad-CAM) ---\n",
    "# =============================================================================\n",
    "\n",
    "def generate_feature_map_info(npz_file_path):\n",
    "    \"\"\"\n",
    "    Dynamically generates feature layout information (color mask, names, colors)\n",
    "    by inspecting the contents of an .npz file.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(npz_file_path):\n",
    "        print(f\"Error: File not found at '{npz_file_path}'\")\n",
    "        return None\n",
    "\n",
    "    feature_layout = {}\n",
    "    print(f\"Inspecting file for feature map info: {npz_file_path}\")\n",
    "\n",
    "    with np.load(npz_file_path) as data:\n",
    "        ignore_keys = {'labels', 'sex', 'age', 'X', 'y'}\n",
    "        feature_keys = [key for key in data.files if key not in ignore_keys]\n",
    "\n",
    "        if not feature_keys:\n",
    "            print(\"Error: No feature arrays found in the .npz file.\")\n",
    "            return None\n",
    "\n",
    "        for key in sorted(feature_keys):\n",
    "            arr_shape = data[key].shape\n",
    "\n",
    "            if len(arr_shape) == 1:\n",
    "                num_rows = 1\n",
    "            elif len(arr_shape) >= 2:\n",
    "                num_rows = arr_shape[1]\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            feature_layout[key] = num_rows\n",
    "\n",
    "    colors = plt.get_cmap('Paired', len(feature_layout))\n",
    "    feature_names = list(feature_layout.keys())\n",
    "    total_rows = sum(feature_layout.values())\n",
    "    time_steps_for_visual = total_rows\n",
    "\n",
    "    color_mask = np.zeros((total_rows, time_steps_for_visual), dtype=int)\n",
    "\n",
    "    current_row = 0\n",
    "    for i, (name, num_rows) in enumerate(feature_layout.items()):\n",
    "        color_mask[current_row : current_row + num_rows, :] = i\n",
    "        current_row += num_rows\n",
    "\n",
    "    legend_patches = [mpatches.Patch(color=colors(i), label=f\"{name} ({feature_layout[name]} rows)\")\n",
    "                      for i, name in enumerate(feature_names)]\n",
    "\n",
    "    return {\n",
    "        'color_mask': color_mask,\n",
    "        'feature_layout': feature_layout,\n",
    "        'feature_names': feature_names,\n",
    "        'colors': colors,\n",
    "        'legend_patches': legend_patches,\n",
    "        'total_rows': total_rows\n",
    "    }\n",
    "\n",
    "def run_gradcam_analysis(model, X_test, y_test, output_path, FEATURES_FILE_PATH, num_samples = 50):\n",
    "    \"\"\"\n",
    "    Run Grad-CAM analysis and plots the average heatmaps for PD and HC alongside the\n",
    "    feature map. The plots are transposed to match SHAP's orientation.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Running Grad-CAM Analysis (3-Plot Version) ---\")\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    parkinson_detector = None\n",
    "    for layer in model.layers:\n",
    "        if 'ParkinsonDetectorModel' in str(type(layer)):\n",
    "            parkinson_detector = layer\n",
    "            break\n",
    "\n",
    "    if parkinson_detector is None:\n",
    "        print(\"ParkinsonDetectorModel not found in the model layers.\")\n",
    "        return\n",
    "\n",
    "    # --- (This entire section for calculating heatmaps is unchanged) ---\n",
    "    def get_conv_and_output(inputs):\n",
    "        x = parkinson_detector.reshape_in(inputs)\n",
    "        x = parkinson_detector.conv1a(x)\n",
    "        x = parkinson_detector.conv1b(x)\n",
    "        x = parkinson_detector.pool1(x)\n",
    "        x = parkinson_detector.drop1(x, training=False)\n",
    "        x = parkinson_detector.conv2a(x)\n",
    "        conv_output = parkinson_detector.conv2b(x)\n",
    "        x = parkinson_detector.pool2(conv_output)\n",
    "        x = parkinson_detector.drop2(x, training=False)\n",
    "        cnn_flat = parkinson_detector.flatten_cnn(x)\n",
    "        shape = tf.shape(x)\n",
    "        sequence = tf.reshape(x, [-1, shape[1] * shape[2], shape[3]])\n",
    "        att_out = parkinson_detector.attention(query=sequence, key=sequence, value=sequence)\n",
    "        att_flat = parkinson_detector.flatten_att(att_out)\n",
    "        lstm_seq = parkinson_detector.lstm1(sequence)\n",
    "        lstm_out = parkinson_detector.lstm2(lstm_seq)\n",
    "        lstm_out = parkinson_detector.drop_lstm(lstm_out, training=False)\n",
    "        concatenated = parkinson_detector.concat([cnn_flat, att_flat, lstm_out])\n",
    "        bottleneck = parkinson_detector.dense_bottleneck(concatenated)\n",
    "        final_output = parkinson_detector.dense_output(bottleneck)\n",
    "        return conv_output, final_output\n",
    "\n",
    "    parkinson_indices = np.where(y_test == 1)[0]\n",
    "    healthy_indices = np.where(y_test == 0)[0]\n",
    "    selected_pd_indices = list(np.random.choice(parkinson_indices, min(num_samples, len(parkinson_indices)), replace=False))\n",
    "    selected_hc_indices = list(np.random.choice(healthy_indices, min(num_samples, len(healthy_indices)), replace=False))\n",
    "    print(f\"Selected {len(selected_pd_indices)} PD and {len(selected_hc_indices)} HC samples for Grad-CAM.\")\n",
    "\n",
    "    tp_heatmaps, tn_heatmaps = [], []\n",
    "    for i in tqdm(selected_pd_indices, desc=\"Grad-CAM PD Progress\"):\n",
    "        img = X_test[i:i+1]\n",
    "        with tf.GradientTape() as tape:\n",
    "            img_tensor = tf.cast(img, tf.float32)\n",
    "            tape.watch(img_tensor)\n",
    "            conv_outputs, preds = get_conv_and_output(img_tensor)\n",
    "            loss = preds[:, 0]\n",
    "        grads = tape.gradient(loss, conv_outputs)\n",
    "        pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "        conv_outputs_np = conv_outputs[0].numpy()\n",
    "        pooled_grads_np = pooled_grads.numpy()\n",
    "        heatmap = np.sum(pooled_grads_np * conv_outputs_np, axis=-1)\n",
    "        heatmap = np.maximum(heatmap, 0)\n",
    "        heatmap /= (heatmap.max() + 1e-10)\n",
    "        tp_heatmaps.append(heatmap)\n",
    "\n",
    "    for i in tqdm(selected_hc_indices, desc=\"Grad-CAM HC Progress\"):\n",
    "        img = X_test[i:i+1]\n",
    "        with tf.GradientTape() as tape:\n",
    "            img_tensor = tf.cast(img, tf.float32)\n",
    "            tape.watch(img_tensor)\n",
    "            conv_outputs, preds = get_conv_and_output(img_tensor)\n",
    "            loss = preds[:, 0]\n",
    "        grads = tape.gradient(loss, conv_outputs)\n",
    "        pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "        conv_outputs_np = conv_outputs[0].numpy()\n",
    "        pooled_grads_np = pooled_grads.numpy()\n",
    "        heatmap = np.sum(pooled_grads_np * conv_outputs_np, axis=-1)\n",
    "        heatmap = np.maximum(heatmap, 0)\n",
    "        heatmap /= (heatmap.max() + 1e-10)\n",
    "        tn_heatmaps.append(heatmap)\n",
    "\n",
    "    avg_pd_heatmap = np.mean(tp_heatmaps, axis=0) if tp_heatmaps else np.zeros((X_test.shape[1], X_test.shape[2]))\n",
    "    avg_hc_heatmap = np.mean(tn_heatmaps, axis=0) if tn_heatmaps else np.zeros((X_test.shape[1], X_test.shape[2]))\n",
    "    original_feature_rows, original_time_steps = X_test.shape[1], X_test.shape[2]\n",
    "    upscaled_avg_pd_heatmap = resize(avg_pd_heatmap, (original_feature_rows, original_time_steps), order=3, mode='reflect', anti_aliasing=True)\n",
    "    upscaled_avg_hc_heatmap = resize(avg_hc_heatmap, (original_feature_rows, original_time_steps), order=3, mode='reflect', anti_aliasing=True)\n",
    "    # --- (End of unchanged section) ---\n",
    "\n",
    "    # --- Plotting: 3 plots in a single row (WITH SWAPPED AXES) ---\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 7)) # <<< MODIFIED: Changed to 3 subplots\n",
    "    fig.suptitle(\"Grad-CAM: Model Attention and Feature Layout\", fontsize=16, fontweight='bold')\n",
    "\n",
    "    # --- 1. Parkinson's - Upscaled Heatmap (Transposed) ---\n",
    "    ax_pd_heatmap = axes[0]\n",
    "    im_pd_hm = ax_pd_heatmap.imshow(upscaled_avg_pd_heatmap.T, cmap='jet', aspect='auto', origin='upper')\n",
    "    ax_pd_heatmap.set_title(f'Avg. PD Attention\\n({len(selected_pd_indices)} samples)')\n",
    "    ax_pd_heatmap.set_ylabel(\"Time Steps\") # <<< MODIFIED\n",
    "    ax_pd_heatmap.set_xlabel(\"Feature Rows\") # <<< MODIFIED\n",
    "    divider_pd = make_axes_locatable(ax_pd_heatmap)\n",
    "    cax_pd = divider_pd.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    fig.colorbar(im_pd_hm, cax=cax_pd)\n",
    "\n",
    "    # --- 2. Healthy - Upscaled Heatmap (Transposed) ---\n",
    "    ax_hc_heatmap = axes[1]\n",
    "    im_hc_hm = ax_hc_heatmap.imshow(upscaled_avg_hc_heatmap.T, cmap='jet', aspect='auto', origin='upper')\n",
    "    ax_hc_heatmap.set_title(f'Avg. Healthy Attention\\n({len(selected_hc_indices)} samples)')\n",
    "    ax_hc_heatmap.set_xlabel(\"Feature Rows\") # <<< MODIFIED\n",
    "    ax_hc_heatmap.set_yticklabels([]) # Hide y-axis labels to avoid repetition\n",
    "    divider_hc = make_axes_locatable(ax_hc_heatmap)\n",
    "    cax_hc = divider_hc.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    fig.colorbar(im_hc_hm, cax=cax_hc)\n",
    "\n",
    "    # --- 3. Feature Map (Transposed) ---\n",
    "    ax_feature_map = axes[2]\n",
    "    feature_map_info = generate_feature_map_info(FEATURES_FILE_PATH)\n",
    "    if feature_map_info:\n",
    "        legend_colors = [patch.get_facecolor() for patch in feature_map_info['legend_patches']]\n",
    "        cmap = ListedColormap(legend_colors)\n",
    "        transposed_data = feature_map_info['color_mask'].T\n",
    "        im_feat = ax_feature_map.imshow(transposed_data, cmap=cmap, aspect='auto', interpolation='nearest', origin='upper')\n",
    "        ax_feature_map.set_title(\"Feature Layout\")\n",
    "        ax_feature_map.set_xlabel(\"Features\")\n",
    "        ax_feature_map.set_yticklabels([]) # Hide y-axis labels\n",
    "        fig.legend(handles=feature_map_info['legend_patches'], loc='center left', bbox_to_anchor=(0.93, 0.5), borderaxespad=0.)\n",
    "    else:\n",
    "        ax_feature_map.set_visible(False)\n",
    "\n",
    "    # --- Final Save ---\n",
    "    plt.tight_layout(rect=[0, 0, 0.93, 0.93])\n",
    "    save_path = os.path.join(output_path, \"gradcam_attention_comparison.png\") # <<< MODIFIED filename\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    print(f\"Saved 3-plot Grad-CAM comparison to {save_path}\")\n"
   ],
   "id": "b1406f4bc7124f9a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### SHAP (SHapley Additive exPlanations) Analysis Pipeline\n",
    "\n",
    "This function (`run_full_shap_analysis`) is the core of the explainability module. It leverages SHAP to explain *why* the model makes specific predictions by assigning an importance value to every input feature (time-frequency bin).\n",
    "\n",
    "#### 1. Configuration & Feature Mapping\n",
    "* **Feature Layout:** Before running SHAP, the function calls `generate_feature_map_info` (assumed to be defined elsewhere) to understand the structure of the input data (e.g., which rows correspond to Mel Spectrograms vs. MFCCs). This allows for human-readable labels later.\n",
    "* **Helper Function `feature_name_for_row`:** Maps a numerical row index (e.g., row 15) to its feature name (e.g., \"Mel Spectrogram\").\n",
    "\n",
    "#### 2. Balanced Sample Selection\n",
    "To ensure the explanation is not biased toward the majority class:\n",
    "* The function identifies indices for **Healthy Controls (HC)** and **Parkinson's (PD)** samples in the test set.\n",
    "* It randomly selects a balanced number of samples (default `samples_per_class=50`) from each group.\n",
    "* These samples are concatenated and shuffled to form the `test_samples` batch for explanation.\n",
    "\n",
    "#### 3. SHAP Value Calculation\n",
    "* **Explainer:** Uses `shap.GradientExplainer`, which is efficient for deep learning models. It requires a background dataset (`X_train[:50]`) to serve as a reference point for \"zero\" feature contribution.\n",
    "* **Computation:** Iterates through the `test_samples` batch, computing SHAP values for each sample. The result is a 3D array: `(n_samples, n_features, n_time_steps)`.\n",
    "\n",
    "#### 4. Visualization 1: Global Feature Importance (Bar Chart)\n",
    "* **Ranking:** The function calculates the mean absolute SHAP value for every feature across all samples. This identifies the most influential features globally.\n",
    "* **Plotting:** A bar chart displays the top 20 most important features. Labels are constructed dynamically (e.g., \"Mel Spectrogram T10\") to pinpoint exactly *what* and *when* the model is looking at.\n",
    "\n",
    "#### 5. Visualization 2: Local & Comparative Heatmaps\n",
    "The function generates detailed heatmaps to compare how the model views Healthy vs. Parkinson's samples.\n",
    "* **Average HC/PD Maps:** Computes the average SHAP values for all Healthy samples and all Parkinson's samples separately.\n",
    "* **Difference Map (PD - HC):** This is the most critical visualization. It subtracts the Healthy map from the Parkinson's map.\n",
    "    * **Red Regions:** Features that push the prediction *towards* Parkinson's.\n",
    "    * **Blue Regions:** Features that push the prediction *towards* Healthy.\n",
    "    * **Insight:** This map often reveals the specific biomarkers (e.g., tremors at specific frequencies) the model has learned.\n",
    "* **Transposed Display:** The plotting helper `plot_aligned_heatmap` swaps axes so that **Time** is on the Y-axis and **Features** are on the X-axis, matching standard spectrogram layouts.\n",
    "\n",
    "#### 6. Combined Output\n",
    "Finally, a comprehensive figure is assembled:\n",
    "* **Left:** Global Feature Importance Bar Chart.\n",
    "* **Right:** A 2x2 grid containing the Average HC heatmap, Average PD heatmap, Difference heatmap, and a reference Feature Map legend.\n",
    "* **Statistical Significance (Optional):** If enough samples are available, a t-test is performed to generate a p-value map, highlighting which feature differences are statistically significant."
   ],
   "id": "8288b0bd9fc0f269"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.patches as mpatches\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from matplotlib.colors import ListedColormap\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import ttest_ind\n",
    "import shap\n",
    "from tensorflow.keras.layers import Lambda, Reshape\n",
    "\n",
    "def feature_name_for_row(row_idx, feature_layout):\n",
    "    \"\"\"\n",
    "    feature_layout: ordered dict-like mapping feature_name -> num_rows\n",
    "    returns the feature_name that contains the given row_idx (0-based).\n",
    "    \"\"\"\n",
    "    cum = 0\n",
    "    for name, nrows in feature_layout.items():\n",
    "        if row_idx < cum + nrows:\n",
    "            return name\n",
    "        cum += nrows\n",
    "    return \"Unknown\"\n",
    "\n",
    "def run_full_shap_analysis(model, X_train, X_test, y_test, output_path, features_npz_path, samples_per_class = 50, top_n=20):\n",
    "    \"\"\"\n",
    "    Run SHAP analysis with balanced sample selection.\n",
    "    Produces:\n",
    "      - combined figure: left = global top-N bar (with feature-category labels),\n",
    "                         right = 2x2 grid with average HC, average PD, diff (PD-HC) and feature map.\n",
    "      - also saves per-figure PNGs for backward compatibility.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Running Full SHAP Analysis (updated plotting) ---\")\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    feature_map_info = generate_feature_map_info(features_npz_path)\n",
    "    if feature_map_info is None:\n",
    "        return\n",
    "\n",
    "    legend_patches = feature_map_info['legend_patches']\n",
    "    total_rows = feature_map_info['total_rows']\n",
    "\n",
    "    # Balanced sample selection\n",
    "    healthy_indices = np.where(y_test == 0)[0]\n",
    "    parkinson_indices = np.where(y_test == 1)[0]\n",
    "    num_healthy_to_select = min(samples_per_class, len(healthy_indices))\n",
    "    num_parkinson_to_select = min(samples_per_class, len(parkinson_indices))\n",
    "    print(f\"Attempting to select {num_healthy_to_select} healthy and {num_parkinson_to_select} Parkinson's samples.\")\n",
    "    selected_healthy_indices = np.random.choice(healthy_indices, num_healthy_to_select, replace=False) if num_healthy_to_select>0 else np.array([])\n",
    "    selected_parkinson_indices = np.random.choice(parkinson_indices, num_parkinson_to_select, replace=False) if num_parkinson_to_select>0 else np.array([])\n",
    "    final_indices = np.concatenate([selected_healthy_indices, selected_parkinson_indices]).astype(int)\n",
    "    np.random.shuffle(final_indices)\n",
    "    test_samples = X_test[final_indices]\n",
    "    y_true_samples = y_test[final_indices]\n",
    "\n",
    "    print(f\"Calculating SHAP values for {len(test_samples)} balanced samples...\")\n",
    "    explainer = shap.GradientExplainer(model, X_train[:50].astype(np.float32))\n",
    "    shap_values_list = []\n",
    "    for sample in tqdm(test_samples, desc=\"SHAP Progress\"):\n",
    "        sample_batch = np.expand_dims(sample, axis=0).astype(np.float32)\n",
    "        sv = explainer.shap_values(sample_batch)\n",
    "        if isinstance(sv, list): sv = sv[0]\n",
    "        shap_values_list.append(sv)\n",
    "    shap_values = np.vstack(shap_values_list)  # shape: (n_samples, rows, time)\n",
    "    print(f\"\\nSHAP values shape: {shap_values.shape}, Test samples shape: {test_samples.shape}\")\n",
    "    if shap_values.shape[1] != total_rows:\n",
    "        print(f\"âš ï¸ Warning: SHAP values feature rows ({shap_values.shape[1]}) do not match total_rows from NPZ ({total_rows}).\")\n",
    "    actual_data_time_steps = shap_values.shape[2]\n",
    "\n",
    "    # --- Global Top-N bar with feature-category labels ---\n",
    "    flat_shap = shap_values.reshape(shap_values.shape[0], -1)\n",
    "    mean_abs = np.mean(np.abs(flat_shap), axis=0)\n",
    "    top_idx = np.argsort(mean_abs)[::-1][:top_n]\n",
    "    coords = [np.unravel_index(i, (shap_values.shape[1], shap_values.shape[2])) for i in top_idx]  # (row, time)\n",
    "\n",
    "    # Build labels using feature_map_info\n",
    "    feature_layout = feature_map_info['feature_layout']\n",
    "    # ensure ordered iteration like in generate_feature_map_info\n",
    "    if isinstance(feature_layout, dict):\n",
    "        # ok: keep insertion order\n",
    "        pass\n",
    "\n",
    "    labels = []\n",
    "    for row_idx, t_idx in coords:\n",
    "        fname = feature_name_for_row(row_idx, feature_layout)\n",
    "        # use 1-based time index for readability\n",
    "        labels.append(f\"{fname} T{t_idx+1}\")\n",
    "\n",
    "    # save standalone global bar\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(range(len(top_idx)), mean_abs[top_idx])\n",
    "    plt.xticks(range(len(top_idx)), labels, rotation=45, ha=\"right\")\n",
    "    plt.title(f\"Top-{top_n} Global SHAP Features (by mean absolute SHAP)\")\n",
    "    plt.xlabel(\"Feature Category Ã— Time\")\n",
    "    plt.ylabel(\"Mean |SHAP value|\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_path, \"shap_global_bar.png\"), dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(\"-> Saved 'shap_global_bar.png'\")\n",
    "\n",
    "    # --- helper plotting: aligned heatmap (but with axes swapped/transposed so x/y are reversed consistently) ---\n",
    "    def plot_aligned_heatmap(heatmap_data, title, filename_suffix, cmap, label, vmin=None, vmax=None, ax=None):\n",
    "        \"\"\"\n",
    "        Plots the SHAP heatmap and Feature Map side by side, BUT displays them with swapped axes\n",
    "        (so time runs on y-axis and features on x-axis in the displayed figure) â€” consistent with request to invert axes.\n",
    "        If ax provided, use it for the heatmap; otherwise create a new fig/ax pair and save to disk.\n",
    "        \"\"\"\n",
    "        # transpose to swap axes for display (rows,time) -> (time,rows)\n",
    "        disp_data = heatmap_data.T  # now shape = (time, rows) -> will show time on vertical, features on horizontal\n",
    "\n",
    "        # If ax not given, create fig with both maps; otherwise only plot into provided ax (featuremap handled separately)\n",
    "        created_fig = False\n",
    "        if ax is None:\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(14, 7))\n",
    "            ax_shap, ax_feature_map_sub = axes[0], axes[1]\n",
    "            created_fig = True\n",
    "        else:\n",
    "            ax_shap = ax\n",
    "            # user must plot feature map separately in combined figure\n",
    "\n",
    "        img = ax_shap.imshow(disp_data, cmap=cmap, aspect='auto', interpolation='nearest', vmin=vmin, vmax=vmax)\n",
    "        ax_shap.set_title(title, fontsize=10)\n",
    "        # since we transposed, x-axis is \"Features (rows)\", y-axis is \"Time steps\"\n",
    "        ax_shap.set_xlabel(f\"Feature Rows ({total_rows})\", fontsize=9)\n",
    "        ax_shap.set_ylabel(f\"Time Steps (Actual: {actual_data_time_steps})\", fontsize=9)\n",
    "        # invert axes visually if you still want reversed direction (optional). Commented out but left for clarity:\n",
    "        # ax_shap.invert_xaxis(); ax_shap.invert_yaxis()\n",
    "\n",
    "        if created_fig:\n",
    "            divider = make_axes_locatable(ax_shap)\n",
    "            cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "            fig.colorbar(img, cax=cax, label=label)\n",
    "\n",
    "            # --- Feature Map on right (aligned, transposed to match display orientation) ---\n",
    "            aligned_color_mask = np.zeros((total_rows, actual_data_time_steps), dtype=int)\n",
    "            current_row = 0\n",
    "            for i, (name, num_rows) in enumerate(feature_map_info['feature_layout'].items()):\n",
    "                aligned_color_mask[current_row : current_row + num_rows, :] = i\n",
    "                current_row += num_rows\n",
    "            # transpose to show time x features consistent with heatmap display\n",
    "            transposed_mask = aligned_color_mask.T\n",
    "            legend_colors = [patch.get_facecolor() for patch in feature_map_info['legend_patches']]\n",
    "            cmap_feat = ListedColormap(legend_colors)\n",
    "            ax_feature_map_sub.imshow(transposed_mask, cmap=cmap_feat, aspect='auto', interpolation='nearest')\n",
    "            ax_feature_map_sub.set_title(\"Feature Map (transposed)\", fontsize=10)\n",
    "            ax_feature_map_sub.set_xlabel(\"Feature Index\")\n",
    "            ax_feature_map_sub.set_ylabel(\"Time Steps\")\n",
    "            ax_feature_map_sub.tick_params(axis='x', labelbottom=False)\n",
    "            fig.legend(handles=feature_map_info['legend_patches'], loc='center left', bbox_to_anchor=(0.93, 0.5), fontsize=8)\n",
    "            fig.suptitle(f\"SHAP Analysis: {title}\", fontsize=14, fontweight='bold')\n",
    "            plt.tight_layout(rect=[0, 0, 0.93, 0.95])\n",
    "            plt.savefig(os.path.join(output_path, f\"shap_aligned_{filename_suffix}.png\"), dpi=300)\n",
    "            plt.close(fig)\n",
    "            print(f\"-> Saved 'shap_aligned_{filename_suffix}.png'\")\n",
    "        else:\n",
    "            return img\n",
    "\n",
    "    # compute masks and means\n",
    "    hc_mask, pd_mask = (y_true_samples == 0), (y_true_samples == 1)\n",
    "    hc_mean = None; pd_mean = None\n",
    "    if np.any(hc_mask):\n",
    "        hc_mean = shap_values[hc_mask].mean(axis=0).squeeze()\n",
    "    if np.any(pd_mask):\n",
    "        pd_mean = shap_values[pd_mask].mean(axis=0).squeeze()\n",
    "\n",
    "    # --- Combined figure: left = global bar, right = 2x2 with HC, PD, Diff, FeatureMap ---\n",
    "    import matplotlib.gridspec as gridspec\n",
    "    fig = plt.figure(constrained_layout=False, figsize=(18, 10))\n",
    "    gs = gridspec.GridSpec(2, 3, figure=fig, width_ratios=[1.0, 0.6, 0.6], height_ratios=[1,1], wspace=0.3, hspace=0.25)\n",
    "\n",
    "    # Left: vertical global bar (occupies both rows)\n",
    "    ax_bar = fig.add_subplot(gs[:, 0])\n",
    "    ax_bar.bar(range(len(top_idx)), mean_abs[top_idx])\n",
    "    ax_bar.set_xticks(range(len(top_idx)))\n",
    "    ax_bar.set_xticklabels(labels, rotation=45, ha='right')\n",
    "    ax_bar.set_title(f\"Top-{top_n} Global SHAP Features\")\n",
    "    ax_bar.set_ylabel(\"Mean |SHAP value|\")\n",
    "\n",
    "    # Right-top: Average Healthy\n",
    "    ax_hc = fig.add_subplot(gs[0, 1])\n",
    "    if hc_mean is not None:\n",
    "        # pass ax to plot_aligned_heatmap to only draw the heatmap there (featuremap will be separate)\n",
    "        img_hc = plot_aligned_heatmap(hc_mean, \"Average SHAP - Healthy\", \"tmp_hc\", \"bwr\", \"Mean SHAP\", ax=ax_hc)\n",
    "        # add colorbar for this axis\n",
    "        divider = make_axes_locatable(ax_hc)\n",
    "        cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "        plt.colorbar(img_hc, cax=cax)\n",
    "    else:\n",
    "        ax_hc.text(0.5, 0.5, 'No HC samples', ha='center', va='center', transform=ax_hc.transAxes)\n",
    "\n",
    "    # Right-middle: Average Parkinson\n",
    "    ax_pd = fig.add_subplot(gs[0, 2])\n",
    "    if pd_mean is not None:\n",
    "        img_pd = plot_aligned_heatmap(pd_mean, \"Average SHAP - Parkinson\", \"tmp_pd\", \"bwr\", \"Mean SHAP\", ax=ax_pd)\n",
    "        divider = make_axes_locatable(ax_pd)\n",
    "        cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "        plt.colorbar(img_pd, cax=cax)\n",
    "    else:\n",
    "        ax_pd.text(0.5, 0.5, 'No PD samples', ha='center', va='center', transform=ax_pd.transAxes)\n",
    "\n",
    "    # Right-bottom-left: Difference map (PD - HC)\n",
    "    ax_diff = fig.add_subplot(gs[1, 1])\n",
    "    if (hc_mean is not None) and (pd_mean is not None):\n",
    "        diff_map = pd_mean - hc_mean\n",
    "        max_abs_diff = np.max(np.abs(diff_map))\n",
    "        img_diff = plot_aligned_heatmap(diff_map, \"SHAP Diff (PD - HC)\", \"tmp_diff\", \"seismic\", \"Î” SHAP\", vmin=-max_abs_diff, vmax=max_abs_diff, ax=ax_diff)\n",
    "        divider = make_axes_locatable(ax_diff)\n",
    "        cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "        plt.colorbar(img_diff, cax=cax)\n",
    "    else:\n",
    "        ax_diff.text(0.5, 0.5, 'Insufficient data for diff', ha='center', va='center', transform=ax_diff.transAxes)\n",
    "\n",
    "    # Right-bottom-right: Feature map (transposed to match heatmaps)\n",
    "    ax_feat = fig.add_subplot(gs[1, 2])\n",
    "    aligned_color_mask = np.zeros((total_rows, actual_data_time_steps), dtype=int)\n",
    "    current_row = 0\n",
    "    for i, (name, num_rows) in enumerate(feature_map_info['feature_layout'].items()):\n",
    "        aligned_color_mask[current_row : current_row + num_rows, :] = i\n",
    "        current_row += num_rows\n",
    "    transposed_mask = aligned_color_mask.T\n",
    "    legend_colors = [patch.get_facecolor() for patch in feature_map_info['legend_patches']]\n",
    "    cmap_feat = ListedColormap(legend_colors)\n",
    "    im_feat = ax_feat.imshow(transposed_mask, cmap=cmap_feat, aspect='auto', interpolation='nearest')\n",
    "    ax_feat.set_title(\"Feature Map (transposed)\", fontsize=10)\n",
    "    ax_feat.set_xlabel(\"Feature Index\")\n",
    "    ax_feat.set_ylabel(\"Time Steps\")\n",
    "    # Legend on right of figure\n",
    "    fig.legend(handles=feature_map_info['legend_patches'], loc='center left', bbox_to_anchor=(0.93, 0.5), fontsize=8)\n",
    "\n",
    "    # save combined\n",
    "    plt.suptitle(\"SHAP Global + Local (with Feature Map)\", fontsize=16, fontweight='bold')\n",
    "    plt.savefig(os.path.join(output_path, \"shap_combined_global_local.png\"), dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "    print(\"-> Saved 'shap_combined_global_local.png'\")\n",
    "\n",
    "    # optional: generate significance map if enough samples\n",
    "    if np.any(hc_mask) and np.any(pd_mask) and np.sum(hc_mask) > 1 and np.sum(pd_mask) > 1:\n",
    "        hc_vals = shap_values[hc_mask].reshape(np.sum(hc_mask), -1)\n",
    "        pd_vals = shap_values[pd_mask].reshape(np.sum(pd_mask), -1)\n",
    "        t_stat, p_vals = ttest_ind(pd_vals, hc_vals, axis=0, equal_var=False)\n",
    "        p_map = p_vals.reshape(shap_values.shape[1], shap_values.shape[2])\n",
    "        plot_aligned_heatmap(p_map, \"Statistical Significance Map (PD vs HC)\", \"significance\", \"viridis_r\", \"p-value (t-test)\", vmin=0, vmax=0.05)\n",
    "    else:\n",
    "        print(\"Skipping significance map: Not enough samples for t-test.\")\n",
    "\n",
    "    print(\"\\n--- SHAP Analysis Complete ---\")\n"
   ],
   "id": "15265dfd5766a99b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Main Execution Pipeline\n",
    "\n",
    "This section orchestrates the end-to-end workflow, connecting data loading, model training, evaluation, and post-hoc analysis.\n",
    "\n",
    "#### 1. Data Preparation\n",
    "* **Loading:** Reads the pre-processed feature matrix (`.npz`).\n",
    "* **Splitting:** Performs a **stratified train-test split (80/20)**. Stratification is crucial here to ensure that the ratio of Healthy to Parkinson's patients remains consistent in both sets, preventing bias.\n",
    "\n",
    "#### 2. Model Training\n",
    "* **Initialization:** Builds the hybrid `ParkinsonDetectorModel` based on the input shape.\n",
    "* **Compilation:** Configures the **Adam** optimizer (`lr=0.001`) and **Binary Cross-Entropy** loss. Tracks Accuracy and AUC.\n",
    "* **Fitting:** Trains for **30 epochs** with a batch size of 32. Uses the `checkpoint_cb` to save only the best model weights based on validation performance.\n",
    "\n",
    "#### 3. Feature Space Analysis\n",
    "To validate the model's learning capability, the script performs a \"Separability Analysis\" at two stages:\n",
    "1.  **Input Features:** Analyzes the raw data before processing. Usually shows high overlap between classes.\n",
    "2.  **Dense Layer Inputs:** Extracts the learned representations just before the final classification layer. This typically demonstrates how the model has clustered the classes, aiming for a high **d-prime** separation score.\n",
    "\n",
    "#### 4. Evaluation & Visualization\n",
    "* **Metrics:** Calculates predictions and saves raw probabilities to `evaluation.csv`.\n",
    "* **Plotting:** Generates a comprehensive square-aspect figure containing the Confusion Matrix, ROC Curve, and Training History (Loss/AUC curves) for a complete performance snapshot.\n",
    "\n",
    "#### 5. Explainability (XAI)\n",
    "Finally, the script loads the best saved model state to run computationally expensive interpretability checks:\n",
    "* **SHAP:** quantifies feature importance.\n",
    "* **Grad-CAM:** visualizes spatial attention maps."
   ],
   "id": "5d0f6af121007791"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# --- Main Execution ---\n",
    "# =============================================================================\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    X, y= load_data(FEATURES_FILE_PATH)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    print(f\"\\nData split into training ({len(y_train)}) and testing ({len(y_test)}) sets.\")\n",
    "\n",
    "    model = build_model(input_shape=(X_train.shape[1], X_train.shape[2]))\n",
    "    model.summary()\n",
    "    optimizer = Adam(learning_rate=LEARNING_RATE)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])\n",
    "\n",
    "    print(\"\\n--- Starting model training ---\")\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_test, y_test),\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        callbacks=[checkpoint_cb],\n",
    "        verbose=1\n",
    "    )\n",
    "    print(\"--- Model training finished ---\")\n",
    "\n",
    "    pd.DataFrame(history.history).to_csv(HISTORY_SAVE_PATH, index_label='epoch')\n",
    "    print(f\"\\nTraining history saved to '{HISTORY_SAVE_PATH}'\")\n",
    "    available_samples = len(X_test)\n",
    "    requested_samples = min(200, available_samples)  # Use the smaller of the two\n",
    "    extract_dense_layer_input_features(model, X_test, y_test, MODEL_PATH, requested_samples)\n",
    "\n",
    "\n",
    "    print(\"\\n--- Start evaluating modrl ---\")\n",
    "    y_pred_probabilities = model.predict(X_test)\n",
    "    save_metrics_to_csv(y_test, y_pred_probabilities, EVALUATION_FILE_PATH)\n",
    "    # --- 2. Analyze Input Feature Separability ---\n",
    "    # This uses the original X_test data before it's fed to the model\n",
    "    analyze_feature_separability(\n",
    "        features=X_test,\n",
    "        labels=y_test,\n",
    "        title_prefix=\"Input Features\",\n",
    "        output_path=MODEL_PATH\n",
    "    )\n",
    "\n",
    "    # --- 3. Analyze Dense Layer Input Feature Separability ---\n",
    "    # First, load the features that you extracted earlier\n",
    "    dense_features_path = os.path.join(MODEL_PATH, \"dense_layer_input_features.npz\")\n",
    "    if os.path.exists(dense_features_path):\n",
    "        with np.load(dense_features_path) as data:\n",
    "            dense_features = data['concatenated_features']\n",
    "            dense_labels = data['labels']\n",
    "\n",
    "        # Now, run the analysis on these extracted features\n",
    "        analyze_feature_separability(\n",
    "            features=dense_features,\n",
    "            labels=dense_labels,\n",
    "            title_prefix=\"Dense Layer Input Features\",\n",
    "            output_path=MODEL_PATH\n",
    "        )\n",
    "    else:\n",
    "        print(f\"\\\\nâš ï¸  Warning: Dense layer features not found at {dense_features_path}. Skipping analysis.\")\n",
    "\n",
    "    print(\"\\\\n\" + \"=\"*80)\n",
    "    print(\"âœ… ENHANCED ANALYSIS COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "        # *** CALL THE NEW ALL-IN-ONE FUNCTION HERE ***\n",
    "    plot_full_evaluation_square(history, y_test, y_pred_probabilities, MODEL_PATH)\n",
    "\n",
    "    if os.path.exists(BEST_MODEL_PATH):\n",
    "        print(\"\\n--- Loading best saved model for explainability analysis ---\")\n",
    "        best_model = load_model(BEST_MODEL_PATH, custom_objects={'ParkinsonDetectorModel': ParkinsonDetectorModel})\n",
    "        run_full_shap_analysis(model, X_train, X_test, y_test, SHAP_OUTPUT_PATH, FEATURES_FILE_PATH, 50, 20)\n",
    "\n",
    "    run_gradcam_analysis(best_model, X_test, y_test, GRADCAM_OUTPUT_PATH, FEATURES_FILE_PATH, 50)"
   ],
   "id": "bbbcc6503553228c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### End-to-End Evaluation on Raw Audio\n",
    "\n",
    "This script provides the final \"sanity check\" for the trained model. Unlike the previous scripts that operate on pre-processed feature matrices (`.npz` files), this script simulates a real-world deployment scenario. It loads the saved model and runs it directly against raw `.wav` audio files stored in folders, performing all necessary pre-processing on the fly.\n",
    "\n",
    "#### 1. Configuration & Constants\n",
    "It re-defines the critical Digital Signal Processing (DSP) parameters used during training (`SAMPLE_RATE`, `DURATION_S`, `N_MELS`, etc.). It is crucial that these match the training phase exactly; otherwise, the input shape to the model will be incorrect.\n",
    "\n",
    "#### 2. Model Loading\n",
    "It loads the best-performing model (saved as `best_model.keras`) from the results directory. This confirms that the saved model file is valid and can be deserialized correctly.\n",
    "\n",
    "#### 3. Real-Time Processing Function (`process_and_predict`)\n",
    "This is the core inference engine. For a single raw audio file:\n",
    "1.  **Load:** Reads the audio using `librosa`.\n",
    "2.  **Fix Length:** Pads or trims the audio to exactly 3 seconds.\n",
    "3.  **Feature Extraction:** Computes the Mel Spectrogram and MFCCs.\n",
    "4.  **Formatting:** Stacks the features vertically and adds a batch dimension (shape becomes `(1, 194, 60)`).\n",
    "5.  **Prediction:** Passes this tensor to the model to get a probability score (0.0 to 1.0).\n",
    "\n",
    "#### 4. Batch Processing (`test_files_in_folder`)\n",
    "Iterates through all `.wav` files in a specific directory (e.g., `healthy_control`), runs the inference function on each, and collects the true labels and predicted probabilities.\n",
    "\n",
    "#### 5. Evaluation & Visualization\n",
    "Once predictions are gathered for both healthy and patient folders:\n",
    "* **Metrics:** Calculates Accuracy, Precision, Recall, and F1-score and saves them to `evaluation_on_raw_audio.csv`.\n",
    "* **Poster-Quality Plot:** Uses `plot_enhanced_confusion_matrix` to generate a high-resolution, aesthetically polished confusion matrix. This plot includes percentages and key metrics (Sensitivity/Specificity) directly on the axes, making it ideal for inclusion in research posters or papers."
   ],
   "id": "5f60450cf8e93dbf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow import keras\n",
    "\n",
    "# --- 1. Audio and Feature Extraction Parameters ---\n",
    "SAMPLE_RATE = 16000\n",
    "DURATION_S = 3\n",
    "AUDIO_SAMPLES = SAMPLE_RATE * DURATION_S\n",
    "N_MELS = 30\n",
    "N_MFCC = 30\n",
    "N_FFT = 2048\n",
    "HOP_LENGTH = 512\n",
    "EXPECTED_FRAMES = int(np.ceil(AUDIO_SAMPLES / HOP_LENGTH))\n",
    "\n",
    "# --- 2. Paths to Model and Data ---\n",
    "MODEL_PATH = os.path.join(os.getcwd(), dataset, f\"results_{MODE}_{FEATURE_MODE}\", MODEL_NAME)\n",
    "EVALUATION_FILE_PATH = os.path.join(MODEL_PATH, \"evaluation_on_raw_audio.csv\")\n",
    "CM_PLOT_PATH = os.path.join(MODEL_PATH, \"confusion_matrix_on_raw_audio_poster.png\") # Changed filename\n",
    "\n",
    "model_path = BEST_MODEL_PATH\n",
    "folder_a_path = \"healthy_control\"\n",
    "folder_b_path = \"parkinson_patient\"\n",
    "\n",
    "# --- 3. Load the Model ---\n",
    "try:\n",
    "    model = keras.models.load_model(model_path)\n",
    "    print(\"Model loaded successfully! âœ…\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- 4. Standardized Evaluation and Plotting Functions ---\n",
    "def save_metrics_to_csv(y_true, y_pred_proba, filename, threshold=0.5):\n",
    "    # (This function remains the same as your provided version)\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred_binary = (np.array(y_pred_proba).flatten() > threshold).astype(int)\n",
    "    cm = confusion_matrix(y_true, y_pred_binary)\n",
    "    if cm.shape == (1,1):\n",
    "        tn, fp, fn, tp = (cm[0][0], 0, 0, 0) if y_true[0] == 0 else (0, 0, 0, cm[0][0])\n",
    "    else:\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "    total_samples = cm.sum()\n",
    "    tn_percent = (tn / total_samples) * 100 if total_samples > 0 else 0\n",
    "    fp_percent = (fp / total_samples) * 100 if total_samples > 0 else 0\n",
    "    fn_percent = (fn / total_samples) * 100 if total_samples > 0 else 0\n",
    "    tp_percent = (tp / total_samples) * 100 if total_samples > 0 else 0\n",
    "    precision = precision_score(y_true, y_pred_binary, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred_binary, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred_binary, zero_division=0)\n",
    "    report_data = {'Metric': ['True Positive (TP)', 'True Negative (TN)', 'False Positive (FP)', 'False Negative (FN)', 'Precision', 'Recall (Sensitivity)', 'F1-Score'],'Value': [f\"{tp} ({tp_percent:.2f}%)\", f\"{tn} ({tn_percent:.2f}%)\", f\"{fp} ({fp_percent:.2f}%)\", f\"{fn} ({fn_percent:.2f}%)\", f\"{precision:.4f}\", f\"{recall:.4f}\", f\"{f1:.4f}\"]}\n",
    "    pd.DataFrame(report_data).to_csv(filename, index=False)\n",
    "    print(f\"Evaluation results saved to: {filename}\")\n",
    "\n",
    "# <<< NEW FUNCTION FOR POSTER-QUALITY PLOT >>>\n",
    "def plot_enhanced_confusion_matrix(y_true, y_pred_proba, save_path, threshold=0.5):\n",
    "    \"\"\"Creates a poster-quality confusion matrix with counts, percentages, and metrics.\"\"\"\n",
    "    y_pred_binary = (np.array(y_pred_proba) > threshold).astype(int)\n",
    "    cm = confusion_matrix(y_true, y_pred_binary)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    labels = np.asarray([\n",
    "        [f\"{tn}\\n({cm_percent[0,0]:.2%})\", f\"{fp}\\n({cm_percent[0,1]:.2%})\"],\n",
    "        [f\"{fn}\\n({cm_percent[1,0]:.2%})\", f\"{tp}\\n({cm_percent[1,1]:.2%})\"]\n",
    "    ])\n",
    "\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=labels, fmt='', cmap='Blues', cbar=False,\n",
    "                xticklabels=['Predicted Healthy', 'Predicted Parkinson'],\n",
    "                yticklabels=['Actual Healthy', 'Actual Parkinson'],\n",
    "                annot_kws={\"size\": 22})\n",
    "\n",
    "    plt.title('Confusion Matrix on Raw Audio', fontsize=20, fontweight='bold')\n",
    "    plt.xlabel(f'Specificity: {specificity:.4f}', fontsize=16)\n",
    "    plt.ylabel(f'Sensitivity (Recall): {sensitivity:.4f}', fontsize=16)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=14)\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"Poster-quality confusion matrix saved to: {save_path}\")\n",
    "\n",
    "# --- 5. Corrected Processing and Prediction Functions ---\n",
    "def process_and_predict(file_path, model):\n",
    "    # (This function remains the same as the corrected version)\n",
    "    try:\n",
    "        audio, sr = librosa.load(file_path, sr=SAMPLE_RATE, duration=DURATION_S)\n",
    "        audio = librosa.util.fix_length(audio, size=AUDIO_SAMPLES)\n",
    "        mel_spectrogram = librosa.feature.melspectrogram(y=audio, sr=sr, n_fft=N_FFT, hop_length=HOP_LENGTH, n_mels=N_MELS)\n",
    "        mel_spectrogram = librosa.util.fix_length(mel_spectrogram, size=EXPECTED_FRAMES, axis=1)\n",
    "        y_preemp = librosa.effects.preemphasis(audio)\n",
    "        mfccs = librosa.feature.mfcc(y=y_preemp, sr=sr, n_mfcc=N_MFCC, n_mels=N_MELS, n_fft=N_FFT, hop_length=HOP_LENGTH)\n",
    "        mfccs = librosa.util.fix_length(mfccs, size=EXPECTED_FRAMES, axis=1)\n",
    "        input_data = np.vstack((mel_spectrogram, mfccs))\n",
    "        input_data = np.expand_dims(input_data, axis=0)\n",
    "        prediction = model.predict(input_data, verbose=0)\n",
    "        return prediction[0][0]\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def test_files_in_folder(folder_path, model, true_label):\n",
    "    # (This function remains the same as the corrected version)\n",
    "    if not os.path.exists(folder_path):\n",
    "        return [], []\n",
    "    file_list = [f for f in os.listdir(folder_path) if f.endswith('.wav')]\n",
    "    print(f\"Found {len(file_list)} WAV files in {folder_path}. Processing...\")\n",
    "    true_labels, predicted_probs = [], []\n",
    "    for file_name in file_list:\n",
    "        predicted_prob = process_and_predict(os.path.join(folder_path, file_name), model)\n",
    "        if predicted_prob is not None:\n",
    "            true_labels.append(true_label)\n",
    "            predicted_probs.append(predicted_prob)\n",
    "    return true_labels, predicted_probs\n",
    "\n",
    "# --- 6. Corrected Main Evaluation Logic ---\n",
    "print(\"\\n--- Starting Model Evaluation on Raw Audio ---\")\n",
    "y_true, y_pred_probs = [], []\n",
    "print(\"\\nProcessing 'healthy_control' folder (Label 0)...\")\n",
    "true_a, pred_a = test_files_in_folder(folder_a_path, model, true_label=0)\n",
    "y_true.extend(true_a)\n",
    "y_pred_probs.extend(pred_a)\n",
    "print(\"\\nProcessing 'parkinson_patient' folder (Label 1)...\")\n",
    "true_b, pred_b = test_files_in_folder(folder_b_path, model, true_label=1)\n",
    "y_true.extend(true_b)\n",
    "y_pred_probs.extend(pred_b)\n",
    "print(\"\\nEvaluation complete. âœ…\")\n",
    "\n",
    "# --- 7. Generate and Display Results ---\n",
    "if len(y_true) > 0 and len(y_pred_probs) > 0:\n",
    "    # Save the detailed evaluation CSV\n",
    "    save_metrics_to_csv(y_true, y_pred_probs, EVALUATION_FILE_PATH)\n",
    "\n",
    "    # Generate and save the poster-quality confusion matrix plot\n",
    "    plot_enhanced_confusion_matrix(y_true, y_pred_probs, CM_PLOT_PATH)\n",
    "else:\n",
    "    print(\"Not enough data to generate results. Please check file paths.\")"
   ],
   "id": "46f3b54414c58c2a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
