{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "333bdf7b",
   "metadata": {},
   "source": [
    "### Configuration and Setup for Audio Processing Pipeline\n",
    "\n",
    "This section of the code initializes the environment, loads necessary libraries, and establishes the configuration parameters that drive the entire data processing and feature extraction workflow.\n",
    "\n",
    "#### 1. Imports and Config Loading\n",
    "The script begins by importing standard libraries for data manipulation (`pandas`, `numpy`), file system operations (`os`, `shutil`, `pathlib`), audio processing (`librosa`, `soundfile`), and progress tracking (`tqdm`).\n",
    "\n",
    "It then reads a configuration CSV file (`config_paths.csv`) which serves as a central registry for dataset locations on the local machine. This allows the code to be portable by simply updating the CSV file rather than hardcoding paths.\n",
    "\n",
    "#### 2. Dataset Specific Path Extraction\n",
    "Using the loaded configuration DataFrame (`CONFIG_df`), the script extracts specific root directories and metadata file paths for three distinct datasets:\n",
    "* **Italian Dataset:** Base path and metadata files for Young Healthy Controls (YHC), Elderly Healthy Controls (EHC), and Parkinson's Disease (PD) patients.\n",
    "* **UAMS Dataset:** Base path and audio folders for Healthy Controls (HC) and PD patients.\n",
    "* **mPower Dataset:** Base path and metadata for the large-scale mobile health dataset."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T15:14:46.810083Z",
     "start_time": "2025-09-18T15:14:46.801571Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import shutil\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Load configuration relative to the script location\n",
    "CONFIG_FILE_PATH = \"config/file_paths.csv\"\n",
    "CONFIG_df = pd.read_csv(CONFIG_FILE_PATH).dropna()\n",
    "\n",
    "# Normalize paths for cross-platform compatibility\n",
    "CONFIG_df[\"path\"] = CONFIG_df[\"path\"].apply(lambda x: x.replace(\"\\\\\", \"/\"))\n",
    "\n",
    "# Extract Italian Dataset Paths\n",
    "Italian_BASE_PATH = CONFIG_df.loc[CONFIG_df[\"data\"] == \"Italian_BASE_PATH\", \"path\"].values[0]\n",
    "Italian_YHC_METADATA = CONFIG_df.loc[CONFIG_df[\"data\"] == \"Italian_YHC_METADATA\", \"path\"].values[0]\n",
    "Italian_EHC_METADATA = CONFIG_df.loc[CONFIG_df[\"data\"] == \"Italian_EHC_METADATA\", \"path\"].values[0]\n",
    "Italian_PD_METADATA = CONFIG_df.loc[CONFIG_df[\"data\"] == \"Italian_PD_METADATA\", \"path\"].values[0]\n",
    "\n",
    "# Extract UAMS Dataset Paths\n",
    "UAMS_BASE_PATH = CONFIG_df.loc[CONFIG_df[\"data\"] == \"UAMS_BASE_PATH\", \"path\"].values[0]\n",
    "UAMS_AUDIO_HC_FOLDER = CONFIG_df.loc[CONFIG_df[\"data\"] == \"UAMS_AUDIO_HC_FOLDER\", \"path\"].values[0]\n",
    "UAMS_AUDIO_PD_FOLDER = CONFIG_df.loc[CONFIG_df[\"data\"] == \"UAMS_AUDIO_PD_FOLDER\", \"path\"].values[0]\n",
    "UAMS_METADATA_FILE = CONFIG_df.loc[CONFIG_df[\"data\"] == \"UAMS_METADATA_FILE\", \"path\"].values[0]\n",
    "\n",
    "# Extract mPower Dataset Paths\n",
    "MPOWER_BASE_PATH = CONFIG_df.loc[CONFIG_df[\"data\"] == \"MPOWER_BASE_PATH\", \"path\"].values[0]\n",
    "MPOWER_AUDIO_HC_FOLDER = os.path.join(MPOWER_BASE_PATH, \"HC\")\n",
    "MPOWER_AUDIO_PD_FOLDER = os.path.join(MPOWER_BASE_PATH, \"PwPD\")\n",
    "MPOWER_METADATA_FILE = CONFIG_df.loc[CONFIG_df[\"data\"] == \"MPOWER_METADATA_FILE\", \"path\"].values[0]"
   ],
   "id": "44af9667dd336651"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Argument Parsing and Dynamic Path Setup\n",
    "\n",
    "This segment handles user inputs and prepares the file system structure required for the processing pipeline.\n",
    "\n",
    "#### 1. Command Line Interface (CLI)\n",
    "The script uses `argparse` to allow users to configure the run parameters via command-line arguments, making the script flexible for automation and different experiments.\n",
    "* **Arguments:**\n",
    "    * `--dataset`: Selects the target dataset (`ITALIAN_DATASET`, `UAMS_DATASET`, or `MPOWER_DATASET`).\n",
    "    * `--mode`: Chooses the analysis scope (e.g., `MODE_A` for vowel /a/ only, or `MODE_ALL_VALIDS` for all tasks).\n",
    "    * `--feature_mode`: Determines the feature set to extract.\n",
    "* **Notebook Compatibility:** A check for `ipykernel` allows the script to run seamlessly inside Jupyter Notebooks (where command-line args aren't passed) by falling back to default values.\n",
    "\n",
    "#### 2. Dynamic Environment Configuration\n",
    "Based on the selected `DATASET`, the script dynamically sets up the working environment:\n",
    "* **Root Paths:** Assigns the correct base directories for the chosen dataset.\n",
    "* **Output Paths:** Generates dataset-specific paths for saving processed data and results.\n",
    "* **File Filtering:** For the Italian dataset specifically, it defines `VALID_FILE_PREFIXES` to filter audio files based on the selected `MODE` (e.g., filtering only specific vowels).\n",
    "\n",
    "#### 3. Automatic Directory Initialization\n",
    "To ensure a clean and organized workspace, the code automatically creates the necessary directory hierarchy if it does not already exist:\n",
    "* `original_...`: Stores pre-processed raw audio.\n",
    "* `augmented_...`: Stores augmented audio samples.\n",
    "* `balanced_...`: Stores the final dataset after class balancing.\n",
    "* **Path Standardization:** Finally, it converts all file paths into `pathlib.Path` objects to ensure robust, cross-platform compatibility throughout the pipeline."
   ],
   "id": "ff1b33a342ba8a8d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f456f49d1e32196",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T15:14:46.881880Z",
     "start_time": "2025-09-18T15:14:46.878387Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Constants ---\n",
    "ITALIAN_DATASET = \"ITALIAN_DATASET\"\n",
    "UAMS_DATASET = \"UAMS_DATASET\"\n",
    "MPOWER_DATASET = \"MPOWER_DATASET\"\n",
    "\n",
    "MODE_A = \"A\"\n",
    "MODE_ALL_VALIDS = \"ALL_VALIDS\"\n",
    "\n",
    "FEATURE_MODE_DEFAULT = \"DEFAULT\"\n",
    "FEATURE_MODE_ALL = \"ALL\"\n",
    "FEATURE_MODE_ACOUSTIC = \"ACOUSTIC\"\n",
    "\n",
    "# --- Argument Parsing (Replaces manual selection) ---\n",
    "parser = argparse.ArgumentParser(description=\"Process audio data for MPD-Net\")\n",
    "parser.add_argument(\"--dataset\", type=str, default=ITALIAN_DATASET,\n",
    "                    choices=[ITALIAN_DATASET, UAMS_DATASET, MPOWER_DATASET],\n",
    "                    help=\"Dataset to process\")\n",
    "parser.add_argument(\"--mode\", type=str, default=MODE_A,\n",
    "                    choices=[MODE_A, MODE_ALL_VALIDS],\n",
    "                    help=\"Processing mode (A vowel or All valid types)\")\n",
    "parser.add_argument(\"--feature_mode\", type=str, default=FEATURE_MODE_DEFAULT,\n",
    "                    choices=[FEATURE_MODE_DEFAULT, FEATURE_MODE_ALL, FEATURE_MODE_ACOUSTIC],\n",
    "                    help=\"Feature extraction mode\")\n",
    "\n",
    "# Allow running in interactive mode (like Jupyter) without crashing\n",
    "if 'ipykernel' in sys.modules:\n",
    "    args = parser.parse_args([])\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "\n",
    "DATASET = args.dataset\n",
    "MODE = args.mode\n",
    "FEATURE_MODE = args.feature_mode\n",
    "\n",
    "print(f\"Processing Configuration: Dataset={DATASET}, Mode={MODE}, Features={FEATURE_MODE}\")\n",
    "\n",
    "# --- Dynamic Path Setup ---\n",
    "PROCESSED_DATA_BASE = \"\"\n",
    "VALID_FILE_PREFIXES = \"\"\n",
    "DATASET_ROOT_PATH = \"\"\n",
    "RESULTS_OUTPUT_PATH = \"\"\n",
    "\n",
    "if DATASET == ITALIAN_DATASET: # Changed 'elif' to 'if'\n",
    "    DATASET_ROOT_PATH = Italian_BASE_PATH\n",
    "    PROCESSED_DATA_BASE = os.path.join(os.getcwd(), \"Italian\", \"data\")\n",
    "    RESULTS_OUTPUT_PATH = os.path.join(os.getcwd(), \"Italian\", f\"results_{MODE}_{FEATURE_MODE}\")\n",
    "    VALID_FILE_PREFIXES = (\"B1\", \"B2\", \"D1\", \"D2\", \"FB1\", \"VA1\", \"VA2\",\n",
    "                       \"VE1\", \"VE2\", \"VI1\", \"VI2\", \"VO1\", \"VO2\",\n",
    "                       \"VU1\", \"VU2\", \"PR1\")\n",
    "    if MODE == MODE_A:\n",
    "        VALID_FILE_PREFIXES = (\"VA1\", \"VA2\",)\n",
    "\n",
    "elif DATASET == UAMS_DATASET:\n",
    "    DATASET_ROOT_PATH = UAMS_BASE_PATH\n",
    "    PROCESSED_DATA_BASE = os.path.join(os.getcwd(), \"UAMS\", \"data\")\n",
    "    RESULTS_OUTPUT_PATH = os.path.join(os.getcwd(), \"UAMS\", f\"results_{MODE}_{FEATURE_MODE}\")\n",
    "\n",
    "elif DATASET == MPOWER_DATASET:\n",
    "    DATASET_ROOT_PATH = MPOWER_BASE_PATH\n",
    "    PROCESSED_DATA_BASE = os.path.join(os.getcwd(), \"mPower\", \"data\")\n",
    "    RESULTS_OUTPUT_PATH = os.path.join(os.getcwd(), \"mPower\", f\"results_{MODE}_{FEATURE_MODE}\")\n",
    "\n",
    "# --- Directory Structure Initialization ---\n",
    "ORIGINAL_DATA_PATH = os.path.join(PROCESSED_DATA_BASE, f\"original_{MODE}_{FEATURE_MODE}\")\n",
    "AUGMENTED_DATA_PATH = os.path.join(PROCESSED_DATA_BASE, f\"augmented_{MODE}_{FEATURE_MODE}\")\n",
    "BALANCED_DATA_PATH = os.path.join(PROCESSED_DATA_BASE, f\"balanced_{MODE}_{FEATURE_MODE}\")\n",
    "FEATURES_OUTPUT_PATH = os.path.join(PROCESSED_DATA_BASE, f\"features_{MODE}_{FEATURE_MODE}.npz\")\n",
    "\n",
    "for path in [PROCESSED_DATA_BASE, ORIGINAL_DATA_PATH, AUGMENTED_DATA_PATH,\n",
    "             BALANCED_DATA_PATH, RESULTS_OUTPUT_PATH]:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "# Convert to Path objects for consistency\n",
    "DATASET_ROOT_PATH = Path(DATASET_ROOT_PATH)\n",
    "PROCESSED_DATA_BASE = Path(PROCESSED_DATA_BASE)\n",
    "ORIGINAL_DATA_PATH = Path(ORIGINAL_DATA_PATH)\n",
    "AUGMENTED_DATA_PATH = Path(AUGMENTED_DATA_PATH)\n",
    "BALANCED_DATA_PATH = Path(BALANCED_DATA_PATH)\n",
    "FEATURES_OUTPUT_PATH = Path(FEATURES_OUTPUT_PATH)\n",
    "RESULTS_OUTPUT_PATH = Path(RESULTS_OUTPUT_PATH)\n",
    "\n",
    "MANIFEST_PATH = Path(os.path.join(PROCESSED_DATA_BASE, f\"manifest_{MODE}_{FEATURE_MODE}.csv\"))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Global Configuration & Hyperparameters\n",
    "\n",
    "This section defines the constant parameters that govern the signal processing, feature extraction, and data augmentation pipelines. Centralizing these values ensures consistency across all experiments.\n",
    "\n",
    "#### 1. Class Definitions\n",
    "Defines the binary classification labels for the dataset structure:\n",
    "* `healthy_control`: Directory name for healthy subjects.\n",
    "* `parkinson_patient`: Directory name for subjects with Parkinson's.\n",
    "\n",
    "#### 2. Audio Processing Standards\n",
    "Establishes a uniform format for all input audio to ensure model compatibility:\n",
    "* **Sample Rate:** Standardized to **16 kHz**.\n",
    "* **Duration:** Fixed at **3 seconds**. Audio shorter than this is dropped or padded; longer audio is trimmed.\n",
    "\n",
    "#### 3. Feature Extraction (DSP)\n",
    "Configures the parameters for the Short-Time Fourier Transform (STFT) and spectral features:\n",
    "* **Frame Size:** 30ms windows are used for analysis.\n",
    "* **FFT Window (`N_FFT`):** Set to **2048** samples.\n",
    "* **Hop Length:** **512** samples (defining the overlap between frames).\n",
    "* **Dimensions:** Extracts **30 Mel bands** for spectrograms and **30 MFCCs** for cepstral analysis.\n",
    "\n",
    "#### 4. Augmentation Settings\n",
    "Parameters for synthetic data generation to improve model robustness:\n",
    "* **Pitch Shift:** Set to **2 semitones**. *(Note: This corrects a previous discrepancy where the code calculated 6 semitones, ensuring alignment with the thesis methodology of $\\pm 2$ semitones).*\n",
    "* **Gain (Volume):** Randomly scales amplitude between **0.9x and 1.1x**.\n",
    "* **White Noise:** Adds noise with a factor of **0.1** to simulate environmental conditions."
   ],
   "id": "d7d023f7440822df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# =============================================================================\n",
    "# --- Configuration Parameters ---\n",
    "# =============================================================================\n",
    "# --- Class Definitions ---\n",
    "HEALTHY_CLASS = \"healthy_control\"\n",
    "PARKINSON_CLASS = \"parkinson_patient\"\n",
    "CLASSES = [HEALTHY_CLASS, PARKINSON_CLASS]\n",
    "\n",
    "# --- Audio Settings ---\n",
    "SAMPLE_RATE = 16000\n",
    "DURATION_S = 3\n",
    "AUDIO_SAMPLES = SAMPLE_RATE * DURATION_S\n",
    "\n",
    "# --- Feature Extraction Parameters ---\n",
    "FRAME_DURATION_MS = 30\n",
    "N_FFT = 2048\n",
    "HOP_LENGTH = 512\n",
    "N_MELS = 30\n",
    "N_MFCC = 30\n",
    "\n",
    "# --- Augmentation Settings ---\n",
    "PITCH_SHIFT_SEMITONES = 2\n",
    "RANDOM_GAIN_RANGE = (0.9, 1.1)\n",
    "WHITE_NOISE_FACTOR = 0.1"
   ],
   "id": "6ea20b6e0617f92e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Dataset Preparation and Standardization\n",
    "\n",
    "This module standardizes three distinct datasets—**Italian**, **UAMS**, and **mPower**—into a unified format suitable for machine learning. It handles file organization, metadata linking (age/sex), audio format conversion, and manifest creation.\n",
    "\n",
    "#### 1. Metadata Handling (`load_italian_metadata`)\n",
    "Specifically for the Italian dataset, this function merges metadata from three separate Excel files (Young Healthy, Elderly Healthy, and Parkinson's Patients).\n",
    "* **Composite Key Generation:** It constructs a unique key for each patient by combining their group folder, name, and surname. This ensures 100% accurate matching between audio files and clinical data, resolving naming inconsistencies.\n",
    "\n",
    "#### 2. Italian Dataset Preparation (`prepare_real_dataset_Italian`)\n",
    "This function processes the Italian dataset by iterating through source directories for Healthy Controls (HC) and Parkinson's Patients (PD).\n",
    "* **Filtering:** It selects specific audio tasks (like vowel /a/) based on `valid_prefixes`.\n",
    "* **Linking:** Uses the composite key to fetch age and sex from the merged metadata.\n",
    "* **Manifest:** Saves a clean CSV manifest mapping every copied WAV file to its ground truth labels.\n",
    "\n",
    "#### 3. UAMS Dataset Preparation (`prepare_real_dataset_UAMS`)\n",
    "Handles the UAMS dataset, which uses a different folder structure.\n",
    "* **ID Matching:** Instead of names, it matches audio filenames to Subject IDs in the demographic CSV.\n",
    "* **Organization:** Copies valid WAV files into the standardized `healthy_control` and `parkinson_patient` output folders.\n",
    "\n",
    "#### 4. mPower Dataset Processing (`prepare_real_dataset_mPower`)\n",
    "This function handles the large-scale, crowd-sourced mPower dataset, which presents unique challenges like varying file formats and quality.\n",
    "* **Format Conversion:** Converts raw `.m4a` files (common on iPhones) to standard `.wav` format using `librosa`.\n",
    "* **Silence Removal:** Trims silence from the beginning and end of recordings to isolate the voice.\n",
    "* **Quality Control:** Drops any recordings shorter than 1.5 seconds to ensure sufficient data for analysis.\n",
    "* **Reporting:** Prints statistics on how many files were processed versus dropped due to quality issues.\n",
    "\n",
    "#### 5. Unified Entry Point (`prepare_real_dataset`)\n",
    "This is the main driver function that accepts a `DATASET` flag and routes the execution to the correct preparation function (Italian, UAMS, or mPower). It ensures that regardless of the input source, the output structure (audio files + manifest CSV) is consistent for the subsequent augmentation and training steps."
   ],
   "id": "7b3631dc8d597ea5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8d2d10ce190fb7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T15:14:46.961413Z",
     "start_time": "2025-09-18T15:14:46.953309Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_italian_metadata(yhc_path, ehc_path, pd_path):\n",
    "    \"\"\"\n",
    "    Loads metadata, using the group folder for PD patients to create a unique\n",
    "    composite key for accurate matching.\n",
    "    \"\"\"\n",
    "    print(\"--- Loading and preparing metadata files with composite keys ---\")\n",
    "    try:\n",
    "        # Load Young Healthy Controls\n",
    "        df_yhc = pd.read_excel(yhc_path)\n",
    "        df_yhc['full_name'] = (df_yhc['name'].astype(str) + ' ' + df_yhc['surname'].astype(str)).str.strip().str.upper()\n",
    "        df_yhc['composite_key'] = df_yhc['full_name'] # Key is just the name for HC\n",
    "        df_yhc['composite_key'] = df_yhc['composite_key'] + \"_15 Young Healthy Control\"\n",
    "\n",
    "        # Load Elderly Healthy Controls\n",
    "        df_ehc = pd.read_excel(ehc_path)\n",
    "        df_ehc['full_name'] = (df_ehc['name'].astype(str) + ' ' + df_ehc['surname'].astype(str)).str.strip().str.upper()\n",
    "        df_ehc['composite_key'] = df_ehc['full_name'] # Key is just the name for HC\n",
    "        df_ehc['composite_key'] = df_ehc['composite_key'] + \"_22 Elderly Healthy Control\"\n",
    "\n",
    "        # Load Parkinson's Disease patients\n",
    "        df_pd = pd.read_excel(pd_path)\n",
    "        # --- Process the group folder column ---\n",
    "        # The first column with '1-5' etc. is the group folder\n",
    "        group_col_name = df_pd.columns[0]\n",
    "        df_pd['group_folder'] = df_pd[group_col_name].str.strip()\n",
    "        df_pd['full_name'] = (df_pd['name'].astype(str) + ' ' + df_pd['surname'].astype(str)).str.strip().str.upper()\n",
    "        df_pd['composite_key'] = df_pd['group_folder'] + '_' + df_pd['full_name']\n",
    "        df_pd['composite_key'] = df_pd['composite_key'] + \"_28 People with Parkinson's disease\"\n",
    "\n",
    "        # Combine all data\n",
    "        all_metadata = pd.concat([\n",
    "            df_yhc[['full_name', 'sex', 'age', 'composite_key']],\n",
    "            df_ehc[['full_name', 'sex', 'age', 'composite_key']],\n",
    "            df_pd[['full_name', 'sex', 'age', 'composite_key']]\n",
    "        ], ignore_index=True)\n",
    "\n",
    "        all_metadata['sex'] = all_metadata['sex'].str.strip().str.upper()\n",
    "\n",
    "        print(f\"Successfully loaded and merged metadata for {len(all_metadata)} individuals.\")\n",
    "        return all_metadata\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading metadata: {e}\")\n",
    "        return pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d690df7e127803e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-18T15:14:46.989267Z",
     "start_time": "2025-09-18T15:14:46.965341Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- 7. Data Filtering ---\n",
    "def prepare_real_dataset_Italian(DATASET, root_path, output_path, valid_prefixes=\"\", metadata_df=None, manifest_path=\"\"):\n",
    "    print(\"--- Step 1: Preparing real dataset and creating manifest ---\")\n",
    "    if any(output_path.iterdir()):\n",
    "        print(f\"Original {output_path} directory is already populated. Skipping preparation.\")\n",
    "        return\n",
    "\n",
    "    if metadata_df is None or metadata_df.empty:\n",
    "        raise ValueError(\"Metadata is required for the Italian dataset but was not provided.\")\n",
    "\n",
    "    manifest_data = []\n",
    "\n",
    "    sources = {\n",
    "        HEALTHY_CLASS: [root_path / \"15 Young Healthy Control\", root_path / \"22 Elderly Healthy Control\"],\n",
    "        PARKINSON_CLASS: [root_path / \"28 People with Parkinson's disease\"]\n",
    "    }\n",
    "\n",
    "    for class_name, source_paths in sources.items():\n",
    "        target_path = output_path / class_name\n",
    "        target_path.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"Copying files and gathering metadata for class: {class_name}\")\n",
    "\n",
    "        for source_path in source_paths:\n",
    "            if not source_path.exists():\n",
    "                print(f\"Warning: Source directory not found, skipping: {source_path}\")\n",
    "                continue\n",
    "\n",
    "            for wav_file in source_path.rglob('*.wav'):\n",
    "                if wav_file.name.upper().startswith(valid_prefixes):\n",
    "                    person_name = wav_file.parent.name.strip().upper()\n",
    "                    top_level_folder_name = source_path.name\n",
    "\n",
    "                    if \"28 People with Parkinson's disease\" in top_level_folder_name:\n",
    "                        group_folder = wav_file.parent.parent.name.strip()\n",
    "                        lookup_key = f\"{group_folder}_{person_name}_{top_level_folder_name}\"\n",
    "                    else:\n",
    "                        lookup_key = f\"{person_name}_{top_level_folder_name}\"\n",
    "\n",
    "                    person_data = metadata_df[metadata_df['composite_key'] == lookup_key]\n",
    "\n",
    "                    if not person_data.empty:\n",
    "                        age = person_data['age'].iloc[0]\n",
    "                        sex_val = person_data['sex'].iloc[0]\n",
    "                    else:\n",
    "                        print(f\"Warning: Could not find metadata for key '{lookup_key}'. Using NaN for age/sex.\")\n",
    "                        age = np.nan\n",
    "                        sex_val = np.nan\n",
    "\n",
    "                    manifest_data.append({\n",
    "                        'original_filename': wav_file.stem, 'age': age, 'sex': sex_val\n",
    "                    })\n",
    "                    shutil.copy2(wav_file, target_path / wav_file.name)\n",
    "\n",
    "\n",
    "    manifest_df = pd.DataFrame(manifest_data)\n",
    "    manifest_df.to_csv(manifest_path, index=False)\n",
    "    print(f\"\\nManifest file with {len(manifest_df)} entries saved to {manifest_path}\")\n",
    "\n",
    "def prepare_real_dataset_UAMS(DATASET, root_path, output_path, valid_prefixes=\"\", metadata_df=None, manifest_path=\"\"):\n",
    "    print(\"--- Step 1: Preparing UAMS dataset and creating manifest ---\")\n",
    "    if any(output_path.iterdir()):\n",
    "        print(f\"Original {output_path} directory already populated. Skipping preparation.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Loading UAMS metadata from: {metadata_df}\")\n",
    "    metadata = pd.read_csv(metadata_df)\n",
    "    metadata.columns = metadata.columns.str.strip()\n",
    "    metadata.set_index('Sample ID', inplace=True)\n",
    "\n",
    "    manifest_data = []\n",
    "    sources = {\n",
    "        HEALTHY_CLASS: Path(UAMS_AUDIO_HC_FOLDER),\n",
    "        PARKINSON_CLASS: Path(UAMS_AUDIO_PD_FOLDER)\n",
    "    }\n",
    "\n",
    "    for class_name, source_path in sources.items():\n",
    "        target_path = output_path / class_name\n",
    "        target_path.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"Copying files and gathering metadata for class: {class_name}\")\n",
    "\n",
    "        if not source_path.exists():\n",
    "            print(f\"Warning: Source directory not found, skipping: {source_path}\")\n",
    "            continue\n",
    "\n",
    "        for wav_file in source_path.glob('*.wav'):\n",
    "            sample_id_found = None\n",
    "            for potential_id in metadata.index:\n",
    "                if wav_file.stem.startswith(potential_id):\n",
    "                    sample_id_found = potential_id\n",
    "                    break\n",
    "\n",
    "            if sample_id_found:\n",
    "                person_data = metadata.loc[sample_id_found]\n",
    "                age = person_data['Age']\n",
    "                sex_val = person_data['Sex']\n",
    "            else:\n",
    "                print(f\"Warning: Could not find metadata for file '{wav_file.name}'. Using NaN.\")\n",
    "                age, sex_val = np.nan, np.nan\n",
    "\n",
    "            manifest_data.append({'original_filename': wav_file.stem, 'age': age, 'sex': sex_val})\n",
    "            shutil.copy2(wav_file, target_path / wav_file.name)\n",
    "\n",
    "    manifest_df = pd.DataFrame(manifest_data)\n",
    "    manifest_df.to_csv(manifest_path, index=False)\n",
    "    print(f\"\\nManifest file with {len(manifest_df)} entries saved to {manifest_path}\")\n",
    "\n",
    "def prepare_real_dataset_mPower(DATASET, root_path, output_path, valid_prefixes=\"\", metadata_df=None, manifest_path=\"\"):\n",
    "    print(\"--- Step 1: Preparing mPower dataset, converting to WAV, and creating manifest ---\")\n",
    "    if any(output_path.iterdir()):\n",
    "        print(f\"Original {output_path} directory already populated. Skipping preparation.\")\n",
    "        return\n",
    "\n",
    "    metadata = pd.read_csv(metadata_df)\n",
    "    metadata.columns = metadata.columns.str.strip()\n",
    "    filename_col = 'audio_audio.m4a'\n",
    "\n",
    "    metadata[filename_col] = metadata[filename_col].astype(str)\n",
    "\n",
    "    manifest_data = []\n",
    "    sources = {\n",
    "        HEALTHY_CLASS: Path(MPOWER_AUDIO_HC_FOLDER),\n",
    "        PARKINSON_CLASS: Path(MPOWER_AUDIO_PD_FOLDER)\n",
    "    }\n",
    "\n",
    "    total_files = 0\n",
    "    processed_files = 0\n",
    "    dropped_files = 0\n",
    "    min_duration = 1.5\n",
    "    for class_name, source_path in sources.items():\n",
    "        target_path = output_path / class_name\n",
    "        target_path.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"Processing and converting files for class: {class_name}\")\n",
    "\n",
    "        if not source_path.exists():\n",
    "            print(f\"Warning: Source directory not found, skipping: {source_path}\")\n",
    "            continue\n",
    "\n",
    "        for m4a_file in tqdm(source_path.glob('*.m4a'), desc=class_name):\n",
    "            total_files += 1\n",
    "\n",
    "            person_data = metadata[metadata[filename_col] == m4a_file.stem]\n",
    "\n",
    "            if not person_data.empty:\n",
    "                age = person_data['age'].iloc[0]\n",
    "                sex_val = person_data['gender'].iloc[0]\n",
    "                if sex_val == \"Female\":\n",
    "                    sex_val = \"F\"\n",
    "                elif sex_val == \"Male\":\n",
    "                    sex_val = \"M\"\n",
    "                else:\n",
    "                    sex_val = np.nan\n",
    "            else:\n",
    "                print(f\"Warning: Could not find metadata for file '{m4a_file.stem}'. Using NaN.\")\n",
    "                age, sex_val = np.nan, np.nan\n",
    "\n",
    "            try:\n",
    "                # Load audio file\n",
    "                y, sr = librosa.load(m4a_file, sr=SAMPLE_RATE, mono=True)\n",
    "\n",
    "                # Remove silence from beginning and end\n",
    "                y_trimmed, _ = librosa.effects.trim(y, top_db=20, frame_length=2048, hop_length=512)\n",
    "\n",
    "                # Calculate duration after trimming\n",
    "                duration = len(y_trimmed) / sr\n",
    "\n",
    "                # Check if duration meets minimum requirement\n",
    "                if duration < min_duration:\n",
    "                    print(f\"Dropping {m4a_file.name}: duration {duration:.2f}s < {min_duration}s\")\n",
    "                    dropped_files += 1\n",
    "                    continue\n",
    "\n",
    "                # Only add to manifest and save if duration is acceptable\n",
    "                manifest_data.append({\n",
    "                    'original_filename': m4a_file.stem,\n",
    "                    'age': age,\n",
    "                    'sex': sex_val,\n",
    "                })\n",
    "\n",
    "                # Save the trimmed audio as WAV\n",
    "                wav_filename = m4a_file.stem + '.wav'\n",
    "                sf.write(target_path / wav_filename, y_trimmed, SAMPLE_RATE)\n",
    "                processed_files += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error converting file {m4a_file.name}: {e}\")\n",
    "                dropped_files += 1\n",
    "\n",
    "    # Print statistics\n",
    "    print(f\"\\n=== Processing Statistics ===\")\n",
    "    print(f\"Total files processed: {total_files}\")\n",
    "    print(f\"Files saved: {processed_files}\")\n",
    "    print(f\"Files dropped: {dropped_files}\")\n",
    "    print(f\"Drop rate: {(dropped_files/total_files)*100:.2f}%\" if total_files > 0 else \"Drop rate: 0%\")\n",
    "\n",
    "    manifest_df = pd.DataFrame(manifest_data)\n",
    "    manifest_df.to_csv(manifest_path, index=False)\n",
    "    print(f\"\\nManifest file with {len(manifest_df)} entries saved to {manifest_path}\")\n",
    "\n",
    "# =============================================================================\n",
    "# --- Step 1: Prepare Original Dataset from Source ---\n",
    "# =============================================================================\n",
    "def prepare_real_dataset(DATASET, root_path, output_path, valid_prefixes=\"\", metadata_df=None, manifest_path=\"\"):\n",
    "    \"\"\"\n",
    "    Copies WAV files and creates a manifest CSV linking each file to its metadata (age, sex).\n",
    "    \"\"\"\n",
    "    if DATASET == ITALIAN_DATASET:\n",
    "        prepare_real_dataset_Italian(DATASET, root_path, output_path, valid_prefixes, metadata_df, manifest_path)\n",
    "\n",
    "    elif DATASET == UAMS_DATASET:\n",
    "        prepare_real_dataset_UAMS(DATASET, root_path, output_path, valid_prefixes, metadata_df, manifest_path)\n",
    "\n",
    "    elif DATASET == MPOWER_DATASET:\n",
    "        prepare_real_dataset_mPower(DATASET, root_path, output_path, valid_prefixes, metadata_df, manifest_path)\n",
    "\n",
    "    print(\"Dataset preparation complete.\\n\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Data Augmentation Pipeline\n",
    "\n",
    "This function expands the dataset by generating synthetic variations of the original audio files. Augmentation is critical for preventing overfitting and making the model robust to real-world variations like different microphone sensitivities or background noise.\n",
    "\n",
    "#### 1. Frame-Based Processing Strategy\n",
    "Instead of augmenting the entire signal at once, the code segments the audio into **30ms frames** (matching the feature extraction window).\n",
    "* **Reasoning:** This mimics how the neural network eventually processes the audio (frame-by-frame) and ensures that augmentations like pitch shifting maintain the temporal structure of the signal without introducing artifacts that might occur from stretching the entire waveform.\n",
    "* **Helper Function:** `_reconstruct_from_frames` stitches these augmented frames back together into a continuous time-domain signal for saving.\n",
    "\n",
    "#### 2. Applied Augmentations\n",
    "For every original file, three new versions are created:\n",
    "\n",
    "* **Pitch Shifting (`_aug_pitch`):**\n",
    "    * Shifts the pitch of the voice by a fixed number of semitones (defined by `PITCH_SHIFT_SEMITONES`) without altering the duration of the clip.\n",
    "    * *Purpose:* Simulates different speakers or vocal characteristics.\n",
    "\n",
    "* **Gain Adjustment (`_aug_gain`):**\n",
    "    * Multiplies the signal amplitude by a random factor chosen from `RANDOM_GAIN_RANGE` (e.g., 0.9x to 1.1x).\n",
    "    * *Purpose:* Simulates variations in loudness or microphone distance.\n",
    "\n",
    "* **White Noise Injection (`_aug_noise`):**\n",
    "    * Adds random Gaussian noise to the signal, scaled by `WHITE_NOISE_FACTOR`.\n",
    "    * *Purpose:* Simulates recording in non-ideal, noisy environments (crucial for the mPower dataset).\n",
    "\n",
    "#### 3. Output\n",
    "The augmented files are saved to the `augmented_...` directory with suffixes indicating the transformation type. This effectively quadruples the size of the training set (1 original + 3 augmented versions)."
   ],
   "id": "db7ccd2051e6d37"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece792b469802782",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# --- Step 2: Augment Data ---\n",
    "# =============================================================================\n",
    "def apply_augmentation(original_path, augmented_path):\n",
    "    \"\"\"\n",
    "    Loads, segments, and augments original audio files. Augmentation is\n",
    "    applied to 30ms frames before reconstruction to match methodology.\n",
    "    \"\"\"\n",
    "    print(\"--- Step 2: Applying Data Augmentation ---\")\n",
    "    if augmented_path.exists() and any(augmented_path.iterdir()):\n",
    "        print(\"Augmented data_italian directory already exists. Skipping augmentation.\")\n",
    "        return\n",
    "    augmented_path.mkdir(exist_ok=True)\n",
    "\n",
    "    def _reconstruct_from_frames(frames, total_samples, frame_length, hop_length):\n",
    "        y_reconstructed = np.zeros(total_samples)\n",
    "        for n, frame_idx in enumerate(range(0, total_samples - frame_length + 1, hop_length)):\n",
    "            if n < frames.shape[1]:\n",
    "                y_reconstructed[frame_idx: frame_idx + frame_length] += frames[:, n]\n",
    "        return y_reconstructed\n",
    "\n",
    "    for class_name in CLASSES:\n",
    "        original_class_path = original_path / class_name\n",
    "        augmented_class_path = augmented_path / class_name\n",
    "        augmented_class_path.mkdir(exist_ok=True)\n",
    "\n",
    "        print(f\"Augmenting files for class: {class_name}\")\n",
    "        files_to_process = list(original_class_path.glob('*.wav'))\n",
    "        for file_path in tqdm(files_to_process, desc=class_name):\n",
    "            y, sr = librosa.load(file_path, sr=SAMPLE_RATE, duration=DURATION_S)\n",
    "            y = librosa.util.fix_length(y, size=AUDIO_SAMPLES)\n",
    "\n",
    "            frame_length = int(sr * (FRAME_DURATION_MS / 1000.0))\n",
    "            hop_length_seg = frame_length // 2\n",
    "            frames = librosa.util.frame(y, frame_length=frame_length, hop_length=hop_length_seg)\n",
    "\n",
    "            # 1. Pitch Shift Augmentation\n",
    "            pitch_frames = np.array([librosa.effects.pitch_shift(y=frame, sr=sr, n_steps=PITCH_SHIFT_SEMITONES) for frame in frames.T]).T\n",
    "            y_pitch = _reconstruct_from_frames(pitch_frames, AUDIO_SAMPLES, frame_length, hop_length_seg)\n",
    "            sf.write(augmented_class_path / f\"{file_path.stem}_aug_pitch.wav\", y_pitch, sr)\n",
    "\n",
    "            # 2. Gain Augmentation\n",
    "            gain = random.uniform(*RANDOM_GAIN_RANGE)\n",
    "            y_gain = _reconstruct_from_frames(frames * gain, AUDIO_SAMPLES, frame_length, hop_length_seg)\n",
    "            sf.write(augmented_class_path / f\"{file_path.stem}_aug_gain.wav\", y_gain, sr)\n",
    "\n",
    "            # 3. White Noise Augmentation\n",
    "            noise = np.random.randn(*frames.shape) * WHITE_NOISE_FACTOR\n",
    "            y_noise = _reconstruct_from_frames(frames + noise, AUDIO_SAMPLES, frame_length, hop_length_seg)\n",
    "            sf.write(augmented_class_path / f\"{file_path.stem}_aug_noise.wav\", y_noise, sr)\n",
    "    print(\"Data augmentation complete.\\n\")\n"
   ],
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Dataset Balancing\n",
    "\n",
    "Medical datasets are frequently imbalanced, with more healthy samples than pathological ones. This function corrects that imbalance to prevent the model from becoming biased toward the majority class.\n",
    "\n",
    "#### 1. Data Aggregation\n",
    "First, it aggregates all available data by combining:\n",
    "* The **Original** raw audio files.\n",
    "* The **Augmented** audio files (created in the previous step).\n",
    "\n",
    "#### 2. Class Counting\n",
    "It counts the total number of samples for each class (`healthy_control` vs. `parkinson_patient`) to identify which is the **Majority** and which is the **Minority**.\n",
    "\n",
    "#### 3. Random Oversampling\n",
    "To achieve a perfectly balanced dataset (50/50 split):\n",
    "* It copies all existing files to a new `balanced_...` directory.\n",
    "* It calculates the deficit (`Majority Count` - `Minority Count`).\n",
    "* It randomly selects files from the **Minority Class** and creates duplicate copies (with unique filenames like `oversample_0_...`) until both classes have an equal number of samples.\n",
    "\n",
    "**Outcome:** This ensures the neural network receives an equal number of examples from both classes during training, which is essential for learning a fair decision boundary."
   ],
   "id": "93c5faacc6fd56fc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# --- Step 3: Balance Dataset ---\n",
    "# =============================================================================\n",
    "def balance_data(original_path, augmented_path, balanced_path):\n",
    "    \"\"\"\n",
    "    Balances the dataset by combining original/augmented data_italian and then\n",
    "    oversampling the minority class to match the majority class count.\n",
    "    \"\"\"\n",
    "    print(\"--- Step 3: Balancing Data with Random Oversampling ---\")\n",
    "    if balanced_path.exists() and any(balanced_path.iterdir()):\n",
    "        print(\"Balanced data_italian directory already exists. Skipping balancing.\")\n",
    "        return\n",
    "    balanced_path.mkdir(exist_ok=True)\n",
    "\n",
    "    all_files = {}\n",
    "    for class_name in CLASSES:\n",
    "        files = list((original_path / class_name).glob('*.wav'))\n",
    "        files.extend(list((augmented_path / class_name).glob('*.wav')))\n",
    "        all_files[class_name] = files\n",
    "\n",
    "    counts = {name: len(files) for name, files in all_files.items()}\n",
    "    minority_class = min(counts, key=counts.get)\n",
    "    majority_class = max(counts, key=counts.get)\n",
    "\n",
    "    print(f\"Minority Class: {minority_class} ({counts[minority_class]} samples)\")\n",
    "    print(f\"Majority Class: {majority_class} ({counts[majority_class]} samples)\")\n",
    "\n",
    "    for class_name, files in all_files.items():\n",
    "        class_dir = balanced_path / class_name\n",
    "        class_dir.mkdir(exist_ok=True)\n",
    "        for f in files:\n",
    "            shutil.copy(f, class_dir / f.name)\n",
    "\n",
    "    samples_to_add = counts[majority_class] - counts[minority_class]\n",
    "    if samples_to_add > 0:\n",
    "        print(f\"Oversampling {minority_class} by adding {samples_to_add} samples...\")\n",
    "        minority_files = all_files[minority_class]\n",
    "        files_to_duplicate = random.choices(minority_files, k=samples_to_add)\n",
    "\n",
    "        for i, file_path in enumerate(tqdm(files_to_duplicate)):\n",
    "            shutil.copy(file_path, balanced_path / minority_class / f\"oversample_{i}_{file_path.name}\")\n",
    "    print(\"Data balancing complete.\\n\")"
   ],
   "id": "68baaf76882b10fb",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Feature Extraction & Metadata Compilation\n",
    "\n",
    "This core function converts raw audio waveforms into structured numerical features suitable for training deep learning models. It also integrates critical patient metadata (age, sex) alongside the audio features.\n",
    "\n",
    "#### 1. Manifest Linking\n",
    "The function loads a pre-generated `manifest.csv` file to link each audio file back to its source subject's metadata.\n",
    "* **Fuzzy Matching:** Since filenames might have been altered during augmentation (e.g., `_aug_pitch`), a helper function `get_original_filename` ensures robust matching back to the original patient record.\n",
    "* **Demographics:** Extracts and encodes `Age` (normalized or raw) and `Sex` (0 for Female, 1 for Male).\n",
    "\n",
    "#### 2. Signal Processing Loop\n",
    "It iterates through every WAV file in the balanced dataset:\n",
    "* **Loading:** Reads audio at the standard 16 kHz rate.\n",
    "* **Corrupt File Handling:** Automatically skips empty or unreadable files to prevent crashes.\n",
    "\n",
    "#### 3. Spectral Feature Computation (`librosa`)\n",
    "For each valid file, it extracts two key features:\n",
    "* **Mel Spectrogram:** Captures the energy distribution across Mel frequency bands over time.\n",
    "* **MFCCs:** Computes Mel-Frequency Cepstral Coefficients, often used for capturing vocal tract shape (formants).\n",
    "* **Fix Length:** Ensures all output matrices have identical dimensions (`30 bands` x `94 time steps`) by padding or trimming, which is essential for batch processing in neural networks.\n",
    "\n",
    "#### 4. Saving Output\n",
    "Finally, all extracted features (`mel_spectrogram`, `mfcc`) and labels (`labels`, `age`, `sex`) are packed into a dictionary and saved as a single compressed NumPy file (`.npz`). This format allows for extremely fast loading during model training."
   ],
   "id": "aca823667268742d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# =============================================================================\n",
    "# --- Step 4: Feature Extraction (MODIFIED VERSION) ---\n",
    "# =============================================================================\n",
    "\n",
    "def extract_features(data_path, feature_mode, manifest_path=\"\"):\n",
    "    \"\"\"\n",
    "    Extracts features and metadata, and safely skips any empty or corrupted audio files.\n",
    "    Enhanced to support acoustic feature extraction with individual named features.\n",
    "    Spectral features will be resized to have 30 time steps, but acoustic features remain as scalar values.\n",
    "    FSC feature has been removed. FEATURE_MODE_BASIC has been removed.\n",
    "    \"\"\"\n",
    "    print(\"--- Step 4: Extracting Features ---\")\n",
    "    manifest_df = pd.DataFrame()\n",
    "    if manifest_path and os.path.exists(manifest_path):\n",
    "        manifest_df = pd.read_csv(\n",
    "            manifest_path,\n",
    "            dtype={'original_filename': str}\n",
    "        ).set_index('original_filename')\n",
    "        print(f\"Loaded manifest file from {manifest_path}\")\n",
    "    else:\n",
    "        print(\"Warning: Manifest file not found. Age and sex data will be unavailable.\")\n",
    "\n",
    "    def get_original_filename(mangled_name, manifest_index):\n",
    "        mangled_name = mangled_name.lower()\n",
    "        for original_name in manifest_index:\n",
    "            if str(original_name).lower() in mangled_name:\n",
    "                return original_name\n",
    "        return None\n",
    "\n",
    "    features = {\n",
    "        \"mel_spectrogram\": [],\n",
    "        \"mfcc\": [],\n",
    "        \"labels\": [],\n",
    "        \"sex\": [],\n",
    "        \"age\": []\n",
    "    }\n",
    "\n",
    "    TARGET_TIME_STEPS = 30\n",
    "    expected_frames = 1 + AUDIO_SAMPLES // HOP_LENGTH\n",
    "\n",
    "\n",
    "    for class_idx, class_name in enumerate(CLASSES):\n",
    "        class_path = data_path / class_name\n",
    "        files = list(class_path.glob('*.wav'))\n",
    "        print(f\"Processing {len(files)} files for class: {class_name}\")\n",
    "\n",
    "        for filename in tqdm(files, desc=class_name):\n",
    "            y, sr = librosa.load(filename, sr=SAMPLE_RATE)\n",
    "            if len(y) == 0:\n",
    "                print(f\"Warning: Skipping empty audio file: {filename.name}\")\n",
    "                continue\n",
    "\n",
    "            age, sex, sex_val = -1, -1, None\n",
    "            if not manifest_df.empty:\n",
    "                original_name = get_original_filename(filename.name, manifest_df.index)\n",
    "                if original_name:\n",
    "                    person_data = manifest_df.loc[original_name]\n",
    "                    age, sex_val = person_data['age'], person_data['sex']\n",
    "                    sex = 0 if isinstance(sex_val, str) and sex_val.upper().startswith('F') else 1 if isinstance(sex_val, str) else -1\n",
    "                else:\n",
    "                    print(f\"Warning: Could not find original filename for {filename.name} in manifest.\")\n",
    "\n",
    "            features[\"age\"].append(age)\n",
    "            features[\"sex\"].append(sex)\n",
    "            features[\"labels\"].append(class_idx)\n",
    "\n",
    "            if feature_mode in [FEATURE_MODE_DEFAULT]:\n",
    "                TARGET_TIME_STEPS = 94\n",
    "                expected_frames = 1 + AUDIO_SAMPLES // HOP_LENGTH\n",
    "                mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=N_FFT, hop_length=HOP_LENGTH, n_mels=N_MELS)\n",
    "                features[\"mel_spectrogram\"].append(librosa.util.fix_length(mel_spectrogram, size=expected_frames, axis=1))\n",
    "\n",
    "                y_preemp = librosa.effects.preemphasis(y)\n",
    "                mfccs = librosa.feature.mfcc(y=y_preemp, sr=sr, n_mfcc=N_MFCC, n_mels=N_MELS, n_fft=N_FFT, hop_length=HOP_LENGTH)\n",
    "                features[\"mfcc\"].append(librosa.util.fix_length(mfccs, size=expected_frames, axis=1))\n",
    "\n",
    "    print(\"Feature extraction complete.\")\n",
    "    print(f\"Feature mode: {feature_mode}\")\n",
    "\n",
    "    if feature_mode in [FEATURE_MODE_ALL, FEATURE_MODE_DEFAULT]:\n",
    "        print(f\"Spectral features resized to {TARGET_TIME_STEPS} time steps.\")\n",
    "\n",
    "    return {key: np.array(val) for key, val in features.items()}\n",
    "\n",
    "# =============================================================================\n",
    "# --- Step 5: Save Features ---\n",
    "# =============================================================================\n",
    "def save_features(output_path, **features):\n",
    "    \"\"\"\n",
    "    Saves all extracted feature matrices and labels to a single\n",
    "    compressed NumPy file (.npz).\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Saving Features to {output_path} ---\")\n",
    "    np.savez_compressed(output_path, **features)\n",
    "    print(\"Features saved successfully.\")\n"
   ],
   "id": "51df7af5c7a779c5",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Visualization Utilities\n",
    "\n",
    "Visual verification is crucial in signal processing pipelines to ensure data integrity. These functions generate plots to confirm that augmentation and feature extraction are working as expected.\n",
    "\n",
    "#### 1. Augmentation Verification (`visualize_augmentation_effect`)\n",
    "This function compares an original waveform against its augmented counterpart.\n",
    "* **Process:** It loads a random file from the `original` dataset and its corresponding pitch-shifted version from the `augmented` dataset.\n",
    "* **Plotting:** It creates a side-by-side time-domain plot. This allows a quick visual check to ensure the augmentation process hasn't corrupted the signal or introduced unwanted silence/artifacts.\n",
    "\n",
    "#### 2. Feature Inspection (`visualize_features`)\n",
    "This function generates a grid of plots displaying the extracted features for both a **Healthy Control** and a **Parkinson's Patient**.\n",
    "* **Mel Spectrograms:** Displays the frequency content over time on a Log-Mel scale (using the `magma` colormap). This visualizes the energy distribution.\n",
    "* **MFCCs:** Displays the Mel-Frequency Cepstral Coefficients over time (using the `coolwarm` colormap). This visualizes the spectral envelope and timbral characteristics.\n",
    "* **Purpose:** These plots serve as a sanity check to verify that the spectral features are being computed correctly and have the expected dimensions before training begins."
   ],
   "id": "5c535f4f68c0f16"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# =============================================================================\n",
    "# --- Visualization Functions ---\n",
    "# =============================================================================\n",
    "def visualize_augmentation_effect(original_path, augmented_path, output_path, mode, feature_mode):\n",
    "    \"\"\"\n",
    "    Loads and plots one original and its augmented version.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Visualizing Augmentation Effect ---\")\n",
    "    try:\n",
    "        hc_path = original_path / HEALTHY_CLASS\n",
    "        original_file = next(hc_path.glob('*.wav'), None)\n",
    "        if not original_file:\n",
    "            print(\"No original file found for visualization.\")\n",
    "            return\n",
    "\n",
    "        augmented_file = augmented_path / HEALTHY_CLASS / f\"{original_file.stem}_aug_pitch.wav\"\n",
    "        if not augmented_file.exists():\n",
    "            print(f\"Augmented file not found: {augmented_file}\")\n",
    "            return\n",
    "\n",
    "        y_orig, sr = librosa.load(original_file, sr=SAMPLE_RATE, duration=DURATION_S)\n",
    "        y_orig = librosa.util.fix_length(y_orig, size=AUDIO_SAMPLES)\n",
    "        y_aug, _ = librosa.load(augmented_file, sr=SAMPLE_RATE)\n",
    "\n",
    "        fig, axs = plt.subplots(2, 1, figsize=(12, 8), sharex=True, sharey=True)\n",
    "        fig.suptitle('Augmentation Effect on a Sample Waveform', fontsize=16)\n",
    "        librosa.display.waveshow(y_orig, sr=sr, ax=axs[0], color='slateblue')\n",
    "        axs[0].set_title(f'Original Audio: {original_file.name}')\n",
    "        librosa.display.waveshow(y_aug, sr=sr, ax=axs[1], color='peru')\n",
    "        axs[1].set_title('Augmented Audio (Pitch Shift)')\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        plt.savefig(os.path.join(output_path, f\"augmentation_visualization_{mode}_{feature_mode}.png\"))\n",
    "        print(\"Saved 'augmentation_visualization.png'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not generate augmentation visualization: {e}\")\n",
    "\n",
    "def visualize_features(balanced_path, output_path, mode, feature_mode):\n",
    "    \"\"\"\n",
    "    Generates and saves visualizations of extracted feature types that\n",
    "    match the style of the reference paper.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Visualizing Features ---\")\n",
    "    try:\n",
    "        samples = {\n",
    "            HEALTHY_CLASS: next((balanced_path / HEALTHY_CLASS).glob('*.wav')),\n",
    "            PARKINSON_CLASS: next((balanced_path / PARKINSON_CLASS).glob('*.wav'))\n",
    "        }\n",
    "\n",
    "        fig, axs = plt.subplots(2, 2, figsize=(12, 9))\n",
    "        fig.suptitle('Feature Extraction Examples', fontsize=16)\n",
    "\n",
    "        class_map = {HEALTHY_CLASS: \"Healthy Control\", PARKINSON_CLASS: \"Parkinson Patient\"}\n",
    "\n",
    "        for i, (class_name, file_path) in enumerate(samples.items()):\n",
    "            title = class_map[class_name]\n",
    "            y, sr = librosa.load(file_path, sr=SAMPLE_RATE)\n",
    "\n",
    "            # Mel Spectrogram\n",
    "            mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=N_FFT, hop_length=HOP_LENGTH, n_mels=N_MELS)\n",
    "            melspec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "            img = librosa.display.specshow(melspec_db, sr=sr, hop_length=HOP_LENGTH, x_axis='time', y_axis='mel', ax=axs[0, i], cmap='magma')\n",
    "            fig.colorbar(img, ax=axs[0, i], format='%+2.0f dB')\n",
    "            axs[0, i].set_title(f'Mel Spectrogram - {title}')\n",
    "            axs[0, i].set_ylabel('Mel Frequency Bins')\n",
    "\n",
    "            # MFCCs\n",
    "            y_preemp = librosa.effects.preemphasis(y)\n",
    "            mfccs = librosa.feature.mfcc(y=y_preemp, sr=sr, n_mfcc=N_MFCC, n_mels=N_MELS, n_fft=N_FFT, hop_length=HOP_LENGTH)\n",
    "            # --- MODIFIED: Use 'coolwarm' colormap and add colorbar ---\n",
    "            img = librosa.display.specshow(mfccs, sr=sr, hop_length=HOP_LENGTH, x_axis='time', ax=axs[1, i], cmap='coolwarm')\n",
    "            fig.colorbar(img, ax=axs[1, i])\n",
    "            axs[1, i].set_title(f'MFCC - {title}')\n",
    "            axs[1, i].set_ylabel('MFCC Coefficients')\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        plt.savefig(os.path.join(output_path, f\"feature_visualizations_{mode}_{feature_mode}.png\"))\n",
    "        print(\"Saved 'feature_visualizations.png'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not generate feature visualizations: {e}\")\n"
   ],
   "id": "ab2111a652cbc745"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Pipeline Summary & Logging\n",
    "\n",
    "This final function aggregates key metrics from the entire data processing workflow into a concise report. This is essential for experiment tracking and ensuring reproducibility.\n",
    "\n",
    "#### 1. Data Counting\n",
    "It calculates and logs the total number of audio files at each stage of the pipeline:\n",
    "* **Original:** The number of valid raw files after filtering.\n",
    "* **Augmented:** The expanded dataset count after pitch shifting, gain adjustment, and noise injection.\n",
    "* **Balanced:** The final count used for training, verifying that the `Healthy` and `Parkinson` classes are equal.\n",
    "\n",
    "#### 2. Feature Dimension Verification\n",
    "It prints and logs the shape of the extracted feature matrices (e.g., `(Samples, 30, 94)` for Mel Spectrograms). This confirms that the data is in the correct format for the neural network input layer.\n",
    "\n",
    "#### 3. CSV Export\n",
    "All collected metrics are compiled into a pandas DataFrame and saved as a CSV file (`data_preparation_...csv`). This provides a permanent record of the exact data conditions used for a specific experiment run."
   ],
   "id": "7aef494d4a5095c1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3004ca26dc791033",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T09:00:23.180405400Z",
     "start_time": "2025-09-18T15:14:47.024701Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_and_save_summary(ORIGINAL_DATA_PATH, HEALTHY_CLASS, PARKINSON_CLASS, AUGMENTED_DATA_PATH, BALANCED_DATA_PATH, MODE, FEATURE_MODE):\n",
    "    summary_data = []\n",
    "    print(\"\\n\\n--- Pipeline Summary ---\")\n",
    "    try:\n",
    "        # Original Data\n",
    "        orig_hc = len(list((ORIGINAL_DATA_PATH / HEALTHY_CLASS).glob('*.wav')))\n",
    "        orig_pd = len(list((ORIGINAL_DATA_PATH / PARKINSON_CLASS).glob('*.wav')))\n",
    "        print(f\"Original Data (Filtered): {orig_hc} HC, {orig_pd} PD\")\n",
    "        summary_data.append({'Category': 'Data Counts', 'Item': 'Original Healthy', 'Value': orig_hc})\n",
    "        summary_data.append({'Category': 'Data Counts', 'Item': 'Original Parkinson', 'Value': orig_pd})\n",
    "\n",
    "        # Augmented Data\n",
    "        aug_hc = len(list((AUGMENTED_DATA_PATH / HEALTHY_CLASS).glob('*.wav')))\n",
    "        aug_pd = len(list((AUGMENTED_DATA_PATH / PARKINSON_CLASS).glob('*.wav')))\n",
    "        print(f\"Augmented Data: {aug_hc} HC, {aug_pd} PD\")\n",
    "        summary_data.append({'Category': 'Data Counts', 'Item': 'Augmented Healthy', 'Value': aug_hc})\n",
    "        summary_data.append({'Category': 'Data Counts', 'Item': 'Augmented Parkinson', 'Value': aug_pd})\n",
    "\n",
    "        # Balanced Data\n",
    "        bal_hc = len(list((BALANCED_DATA_PATH / HEALTHY_CLASS).glob('*.wav')))\n",
    "        bal_pd = len(list((BALANCED_DATA_PATH / PARKINSON_CLASS).glob('*.wav')))\n",
    "        print(f\"Balanced Data: {bal_hc} HC, {bal_pd} PD\")\n",
    "        summary_data.append({'Category': 'Data Counts', 'Item': 'Balanced Healthy', 'Value': bal_hc})\n",
    "        summary_data.append({'Category': 'Data Counts', 'Item': 'Balanced Parkinson', 'Value': bal_pd})\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"Could not generate summary as some data directories are missing.\")\n",
    "\n",
    "    print(\"\\n--- Final Feature Matrix Shapes ---\")\n",
    "    for name, data in all_features.items():\n",
    "        title_name = name.replace('_', ' ').title()\n",
    "        shape_str = str(data.shape)\n",
    "        print(f\"{title_name}: {shape_str}\")\n",
    "        summary_data.append({'Category': 'Feature Shapes', 'Item': title_name, 'Value': shape_str})\n",
    "\n",
    "    if summary_data:\n",
    "        try:\n",
    "            summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "            print(\"\\n--- Summary DataFrame ---\")\n",
    "            print(summary_df.to_string())\n",
    "\n",
    "            summary_df.columns = ['Category', 'Item', 'Value']\n",
    "            summary_df = summary_df.iloc[1:]\n",
    "            summary_df.to_csv(os.path.join(RESULTS_OUTPUT_PATH, f'data_preperation_{MODE}_{FEATURE_MODE}.csv'), index=False, encoding='utf-8')\n",
    "            print(\"\\nUnified summary saved to 'data_preperation.csv'\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\nCould not create or save summary DataFrame. Error: {e}\")\n",
    "\n",
    "    print(\"\\nPipeline finished successfully!\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Main Execution Entry Point\n",
    "\n",
    "This block orchestrates the entire data pipeline by calling the previously defined functions in the correct sequence. It serves as the \"conductor\" for the data processing symphony.\n",
    "\n",
    "#### 1. Dataset-Specific Initialization\n",
    "Based on the `DATASET` constant selected at the beginning of the script, it branches to handle the unique setup requirements for each dataset:\n",
    "* **Italian Dataset:** Specifically loads and merges the complex metadata structure before starting the preparation.\n",
    "* **UAMS & mPower:** Directly initiates preparation using their respective metadata files.\n",
    "\n",
    "#### 2. Sequential Processing Pipeline\n",
    "Once the initial dataset is prepared, the script executes the core processing steps in order:\n",
    "1.  **Augmentation:** Calls `apply_augmentation` to generate synthetic data variants.\n",
    "2.  **Balancing:** Calls `balance_data` to equalize class distribution using the augmented data.\n",
    "3.  **Feature Extraction:** Calls `extract_features` to compute spectrograms and MFCCs from the balanced dataset.\n",
    "4.  **Saving:** Calls `save_features` to persist the final feature set to disk as a `.npz` file.\n",
    "\n",
    "#### 3. Reporting and Visualization\n",
    "Finally, it generates the artifacts needed for analysis and verification:\n",
    "* **Visualizations:** Creates plots for augmentation effects and feature heatmaps.\n",
    "* **Summary:** Generates and saves the final CSV report detailing dataset statistics and feature dimensions."
   ],
   "id": "24ea7135fc4da371"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67acc9bd48bc1d42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T09:00:23.191406300Z",
     "start_time": "2025-09-18T15:14:47.035463Z"
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# --- Main Execution ---\n",
    "# =============================================================================\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    if DATASET == ITALIAN_DATASET:\n",
    "        italian_metadata = pd.DataFrame()\n",
    "        italian_metadata = load_italian_metadata(Italian_YHC_METADATA, Italian_EHC_METADATA, Italian_PD_METADATA)\n",
    "        prepare_real_dataset(DATASET, DATASET_ROOT_PATH, ORIGINAL_DATA_PATH, VALID_FILE_PREFIXES, italian_metadata, MANIFEST_PATH)\n",
    "    elif DATASET == UAMS_DATASET:\n",
    "        prepare_real_dataset(DATASET, DATASET_ROOT_PATH, ORIGINAL_DATA_PATH, VALID_FILE_PREFIXES, Path(UAMS_METADATA_FILE), MANIFEST_PATH)\n",
    "    elif DATASET == MPOWER_DATASET:\n",
    "        prepare_real_dataset(DATASET, DATASET_ROOT_PATH, ORIGINAL_DATA_PATH, VALID_FILE_PREFIXES, Path(MPOWER_METADATA_FILE), MANIFEST_PATH)\n",
    "\n",
    "    apply_augmentation(ORIGINAL_DATA_PATH, AUGMENTED_DATA_PATH)\n",
    "    balance_data(ORIGINAL_DATA_PATH, AUGMENTED_DATA_PATH, BALANCED_DATA_PATH)\n",
    "    all_features = extract_features(BALANCED_DATA_PATH, FEATURE_MODE, MANIFEST_PATH)\n",
    "    save_features(FEATURES_OUTPUT_PATH, **all_features)\n",
    "\n",
    "    # --- Generate Visualizations ---\n",
    "    visualize_augmentation_effect(ORIGINAL_DATA_PATH, AUGMENTED_DATA_PATH, RESULTS_OUTPUT_PATH, MODE, FEATURE_MODE)\n",
    "    visualize_features(BALANCED_DATA_PATH, RESULTS_OUTPUT_PATH, MODE, FEATURE_MODE)\n",
    "    generate_and_save_summary(ORIGINAL_DATA_PATH, HEALTHY_CLASS, PARKINSON_CLASS, AUGMENTED_DATA_PATH, BALANCED_DATA_PATH, MODE, FEATURE_MODE)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
